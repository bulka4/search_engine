{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f1a2392-21c6-41ff-953d-a7de695d2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e4f90-9618-4530-a654-6633609c7362",
   "metadata": {},
   "source": [
    "## preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af5514d9-49a7-4b54-bd34-143ca5a5fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-fr.csv'), engine='python')\n",
    "# df.iloc[:1000].to_csv(os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-fr-small.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "338b8257-cb2d-4942-8353-706e4b10d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-fr-small.csv'), engine='python', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac0a447e-7c34-40fe-a608-7e39f8cdb2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Changing Lives | Changing Society | How It Wor...</td>\n",
       "      <td>Il a transformÃƒÂ© notre vie | Il a transformÃ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Site map</td>\n",
       "      <td>Plan du site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feedback</td>\n",
       "      <td>RÃƒÂ©troaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Credits</td>\n",
       "      <td>CrÃƒÂ©dits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FranÃƒÂ§ais</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>A nova is a star that absorbs matter from a ne...</td>\n",
       "      <td>La matiÃƒÂ¨re absorbÃƒÂ©e finit par rÃƒÂ©chauf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>It is a rare and spectacular event.</td>\n",
       "      <td>Il s'agit d'un phÃƒÂ©nomÃƒÂ¨ne rare et plutÃƒÂ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>In 1977, he became the French editor of the Na...</td>\n",
       "      <td>En 1977, il devient l'ÃƒÂ©diteur francophone d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>In 1978, Lemay began the daunting task of asse...</td>\n",
       "      <td>En 1978, il entreprend la tÃƒÂ¢che colossale d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>He completed the work in 1985 after taking 1,1...</td>\n",
       "      <td>Il achÃƒÂ¨ve son travail en 1985 aprÃƒÂ¨s avoi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    en  \\\n",
       "0    Changing Lives | Changing Society | How It Wor...   \n",
       "1                                             Site map   \n",
       "2                                             Feedback   \n",
       "3                                              Credits   \n",
       "4                                          FranÃƒÂ§ais   \n",
       "..                                                 ...   \n",
       "995  A nova is a star that absorbs matter from a ne...   \n",
       "996                It is a rare and spectacular event.   \n",
       "997  In 1977, he became the French editor of the Na...   \n",
       "998  In 1978, Lemay began the daunting task of asse...   \n",
       "999  He completed the work in 1985 after taking 1,1...   \n",
       "\n",
       "                                                    fr  \n",
       "0    Il a transformÃƒÂ© notre vie | Il a transformÃ...  \n",
       "1                                         Plan du site  \n",
       "2                                       RÃƒÂ©troaction  \n",
       "3                                           CrÃƒÂ©dits  \n",
       "4                                              English  \n",
       "..                                                 ...  \n",
       "995  La matiÃƒÂ¨re absorbÃƒÂ©e finit par rÃƒÂ©chauf...  \n",
       "996  Il s'agit d'un phÃƒÂ©nomÃƒÂ¨ne rare et plutÃƒÂ...  \n",
       "997  En 1977, il devient l'ÃƒÂ©diteur francophone d...  \n",
       "998  En 1978, il entreprend la tÃƒÂ¢che colossale d...  \n",
       "999  Il achÃƒÂ¨ve son travail en 1985 aprÃƒÂ¨s avoi...  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03d3b3b1-0e72-4b3a-b5f4-5f037c18f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_sentences = 1000  # Number of sentences to include in the dataset\n",
    "        self.train_split = 0.9  # Ratio of the training data split\n",
    "\n",
    "    # Fit a tokenizer\n",
    "    def create_tokenizer(self, dataset):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def find_seq_length(self, dataset):\n",
    "        return max(len(seq.split()) for seq in dataset)\n",
    "\n",
    "    def find_vocab_size(self, tokenizer, dataset):\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return len(tokenizer.word_index) + 1\n",
    "\n",
    "    def __call__(self, filename = 'en-fr-small.csv', **kwargs):\n",
    "        # Load a clean dataset\n",
    "        clean_dataset = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), 'data', filename), engine='python', index_col = 0).values\n",
    "\n",
    "        # Reduce dataset size\n",
    "        dataset = clean_dataset[:self.n_sentences, :]\n",
    "\n",
    "        # Include start and end of string tokens\n",
    "        for i in range(dataset[:, 0].size):\n",
    "            dataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <END>\"\n",
    "            dataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <END>\"\n",
    "\n",
    "        # Random shuffle the dataset\n",
    "        np.random.shuffle(dataset)\n",
    "\n",
    "        # Split the dataset\n",
    "        train = dataset[:int(self.n_sentences * self.train_split)]\n",
    "\n",
    "        # Prepare tokenizer for the encoder input\n",
    "        enc_tokenizer = self.create_tokenizer(train[:, 0])\n",
    "        enc_seq_length = self.find_seq_length(train[:, 0])\n",
    "        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "\n",
    "        # Encode and pad the input sequences\n",
    "        trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n",
    "        trainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n",
    "        # trainX = convert_to_tensor(trainX, dtype=int64)\n",
    "        trainX = tf.constant(trainX)\n",
    "\n",
    "        # Prepare tokenizer for the decoder input\n",
    "        dec_tokenizer = self.create_tokenizer(train[:, 1])\n",
    "        dec_seq_length = self.find_seq_length(train[:, 1])\n",
    "        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "\n",
    "        # Encode and pad the input sequences\n",
    "        trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n",
    "        trainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n",
    "        # trainY = convert_to_tensor(trainY, dtype=int64)\n",
    "        trainY = tf.constant(trainY)\n",
    "\n",
    "        return dataset, trainX, trainY, train, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size, enc_tokenizer, dec_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "872c8fcc-e3b3-452b-b919-058b5becede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data = PrepareDataset()\n",
    "dataset, trainX, trainY, train, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size, enc_tokenizer, dec_tokenizer = prep_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ea0e48fa-6039-41fc-8884-2b7b37bf115e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(61,), dtype=int32, numpy=\n",
       "array([  3,   5, 335,  15, 285,   1,  10,  37,   1,  18,   5,  19,   7,\n",
       "       397,  28, 220, 398,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0])>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a88eb08-85a7-4b4b-ad58-4ffad8d2e4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(63,), dtype=int32, numpy=\n",
       "array([  2,   4, 332,  27, 384,  16,  13,   7,  36,  15,  21,   6, 818,\n",
       "        51, 385, 333,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c92215a-48d3-4f08-ab89-ee8a67ce83c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<START> In 1991, for example, the observatory became the first in Canada to use an infrared camera. <END>',\n",
       "       \"<START> En 1991, par exemple, l'observatoire est le premier au Canada ÃƒÂ\\xa0 disposer d'une camÃƒÂ©ra infrarouge. <END>\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00bd9cf2-77fc-46a3-ba04-d2e2ce3c6590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start in 1991 for example the observatory became the first in canada to use an infrared camera end']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tokenizer.sequences_to_texts(trainX[tf.newaxis, 0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "072c839f-3152-4e3b-891d-c85210b47322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"start en 1991 par exemple l'observatoire est le premier au canada ãƒâ\\xa0 disposer d'une camãƒâ©ra infrarouge end\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_tokenizer.sequences_to_texts(trainY[tf.newaxis, 0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f3463-f5dd-4f27-a8b4-7fc2067944d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8987552-387d-4131-ac57-37f3b6c33f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7258d-7477-4447-9764-fb7835780f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433c1d0c-216d-4670-88e3-d2090e9c4895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db15cae-4f70-40cb-ad1c-9fa5dc1d7cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e0dfb-1c67-4150-ab36-364352c07642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4b1c7-3672-4b11-b68b-96f5f4ac8af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03ea5838-36ef-4cd5-9597-57e0f0d7099a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## step by step calculations for transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e44e1070-c82a-489d-97fc-bde0d152cf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.constant(np.random.rand(2, 2)) # (bs, in_seq_len)\n",
    "target = tf.constant(np.random.rand(2, 3)) # (bs, tg_seq_len)\n",
    "encoder_mask = tf.cast(tf.math.not_equal(inputs, 0), tf.float32) # (bs, in_seq_len)\n",
    "decoder_mask = tf.linalg.band_part(tf.ones((tf.shape(target)[0], tf.shape(target)[1])), -1, 0) # (bs, tg_seq_len)\n",
    "encoder_decoder_mask = tf.cast(tf.math.not_equal(inputs, 0), tf.float32) # (bs, tg_seq_len)\n",
    "print(encoder_decoder_mask.shape)\n",
    "\n",
    "encoder_mask = encoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "encoder_decoder_mask = encoder_decoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "decoder_mask = decoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "tfm = Transformer(\n",
    "    num_blocks = 5,\n",
    "    d_model = 20,\n",
    "    num_heads = 5,\n",
    "    hidden_dim = 30,\n",
    "    source_vocab_size = 40,\n",
    "    target_vocab_size = 40,\n",
    "    max_input_len = inputs.shape[1],\n",
    "    max_target_len = target.shape[1]\n",
    ")\n",
    "\n",
    "enc = tfm.encoder\n",
    "dec = tfm.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60e8c834-dec0-460e-a20a-4843212af52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True\n",
    "# inputs.shape = (batch_size, seq_len)\n",
    "seq_len = tf.shape(inputs)[1]\n",
    "inputs = enc.embedding(inputs) \n",
    "inputs *= tf.math.sqrt(tf.cast(enc.d_model, tf.float32))\n",
    "inputs += enc.pos_encoding[:, :seq_len, :]\n",
    "inputs = enc.dropout(inputs, training=training) # inputs.shape = (batch_size, seq_len, embed_dim)\n",
    "\n",
    "for block in enc.blocks:\n",
    "    # mha_output, attn_weights = self.mha(inputs, inputs, inputs, mask)\n",
    "    query = key = value = inputs\n",
    "    mask = encoder_mask\n",
    "    # query, key and value have shape (batch_size, inp_seq_len, embed_dim)\n",
    "    # mask has shape (batch_size, 1, 1, inp_seq_len)\n",
    "    # if mask has shape (batch_size, inp_seq_len) we can change it to proper shape by writing\n",
    "    # mask = mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    query = block.mha.wq(query) # qs, ks and vs have shape (batch_size, inp_seq_len, d_model)\n",
    "    key = block.mha.wk(key)\n",
    "    value = block.mha.wv(value)\n",
    "    \n",
    "    query = block.mha.split_heads(query) # qs, ks and vs have shape (batch_size, num_heads, inp_seq_len, d_head)\n",
    "    key = block.mha.split_heads(key)\n",
    "    value = block.mha.split_heads(value)\n",
    "\n",
    "    # output, attn_weights = self.scaled_dot_product_attention(query, key, value, mask)\n",
    "    # query, key and value have shape (batch_size, num_heads, inp_seq_len, d_head)\n",
    "    # mask has shape (batch_size, 1, 1, inp_seq_len)\n",
    "    key_dim = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    # scaled_scores has shape (batch_size, num_heads, inp_seq_len, inp_seq_len)\n",
    "    scaled_scores = tf.matmul(query, key, transpose_b=True) / np.sqrt(key_dim)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
    "\n",
    "    softmax = tf.keras.layers.Softmax()\n",
    "    weights = softmax(scaled_scores) \n",
    "    # output of scaled_dot_product_attention\n",
    "    output = tf.matmul(weights, value)\n",
    "    \n",
    "    output = block.mha.merge_heads(output) # output.shape = (batch_size, inp_seq_len, d_model)\n",
    "    output = block.mha.dense(output) # output.shape = (batch_size, inp_seq_len, d_model)\n",
    "    mha_output = output\n",
    "    \n",
    "    mha_output = block.dropout1(mha_output, training=training)\n",
    "    mha_output = block.layernorm1(inputs + mha_output)\n",
    "\n",
    "    ffn_output = block.ffn(mha_output)\n",
    "    ffn_output = block.dropout2(ffn_output, training=training)\n",
    "    output = block.layernorm2(mha_output + ffn_output)\n",
    "\n",
    "    inputs = output\n",
    "    \n",
    "encoder_output = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed5196f1-9668-429d-a5da-53d1cd3137ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5, 2, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "285f60f1-6703-419d-a0bd-a32f99b136ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2, 20])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7041c0a6-cb04-4f96-a76e-480817354759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_output.shape = (batch_size, inp_seq_len, embed_dim)\n",
    "# target.shape = (batch_size, targ_seq_len)\n",
    "# decoder_mask.shape = encoder_decoder_mask.shape = (batch_size, 1, 1, targ_seq_len)\n",
    "seq_len = tf.shape(target)[1]\n",
    "target = dec.embedding(target)\n",
    "target *= tf.math.sqrt(tf.cast(dec.d_model, tf.float32))\n",
    "target += dec.pos_encoding[:, :seq_len, :]\n",
    "target = dec.dropout(target, training=training) # .shape = (batch_size, targ_seq_len, embed_dim)\n",
    "\n",
    "for block in dec.blocks:\n",
    "    # encoder_output.shape = (batch_size, targ_seq_len, embed_dim)\n",
    "    # target.shape = (batch_size, tar_seq_len, embed_dim)\n",
    "    # decoder_mask.shape = encoder_decoder_mask.shape = (batch_size, 1, 1, tar_seq_len)\n",
    "    # encoder_decoder_mask.shape = encoder_decoder_mask.shape = (batch_size, 1, 1, targ_seq_len)\n",
    "\n",
    "    # mha_output1, attn_weights = block.mha1(target, target, target, decoder_mask)\n",
    "    query = key = value = target\n",
    "    mask = decoder_mask\n",
    "    # query, key and value have shape (batch_size, targ_seq_len, embed_dim)\n",
    "    # mask has shape (batch_size, 1, 1, targ_seq_len)\n",
    "\n",
    "    query = block.mha1.wq(query) # qs, ks and vs have shape (batch_size, targ_seq_len, d_model)\n",
    "    key = block.mha1.wk(key)\n",
    "    value = block.mha1.wv(value)\n",
    "    \n",
    "    query = block.mha1.split_heads(query) # qs, ks and vs have shape (batch_size, num_heads, targ_seq_len, d_head)\n",
    "    key = block.mha1.split_heads(key)\n",
    "    value = block.mha1.split_heads(value)\n",
    "\n",
    "    # output, attn_weights = self.scaled_dot_product_attention(query, key, value, mask)\n",
    "    # query, key and value have shape (batch_size, num_heads, targ_seq_len, d_head)\n",
    "    # mask has shape (batch_size, 1, 1, targ_seq_len)\n",
    "    key_dim = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    # scaled_scores has shape (batch_size, num_heads, targ_seq_len, targ_seq_len)\n",
    "    scaled_scores = tf.matmul(query, key, transpose_b=True) / np.sqrt(key_dim)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
    "\n",
    "    softmax = tf.keras.layers.Softmax()\n",
    "    weights = softmax(scaled_scores) \n",
    "    # output of scaled_dot_product_attention\n",
    "    output = tf.matmul(weights, value)\n",
    "    \n",
    "    output = block.mha1.merge_heads(output) # output.shape = (batch_size, targ_seq_len, d_model)\n",
    "    output = block.mha1.dense(output) # output.shape = (batch_size, targ_seq_len, d_model)\n",
    "    mha_output1 = output\n",
    "    \n",
    "    mha_output1 = block.dropout1(mha_output1, training=training)\n",
    "    mha_output1 = block.layernorm1(mha_output1 + target) # mha_output1.shape = (batch_size, tar_seq_len, embed_dim)\n",
    "\n",
    "    # mha_output2, attn_weights = block.mha2(mha_output1, encoder_output, encoder_output, encoder_decoder_mask)\n",
    "    query = mha_output1\n",
    "    key = value = encoder_output\n",
    "    mask = encoder_decoder_mask\n",
    "    # query, key and value have shape (batch_size, targ_seq_len, embed_dim)\n",
    "    # mask has shape (batch_size, 1, 1, targ_seq_len)\n",
    "\n",
    "    query = block.mha2.wq(query) # qs, ks and vs have shape (batch_size, targ_seq_len, d_model)\n",
    "    key = block.mha2.wk(key)\n",
    "    value = block.mha2.wv(value)\n",
    "    \n",
    "    query = block.mha2.split_heads(query) # qs, ks and vs have shape (batch_size, num_heads, targ_seq_len, d_head)\n",
    "    key = block.mha2.split_heads(key)\n",
    "    value = block.mha2.split_heads(value)\n",
    "\n",
    "    # output, attn_weights = self.scaled_dot_product_attention(query, key, value, mask)\n",
    "    # query, key and value have shape (batch_size, num_heads, targ_seq_len, d_head)\n",
    "    # mask has shape (batch_size, 1, 1, targ_seq_len)\n",
    "    key_dim = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    # scaled_scores has shape (batch_size, num_heads, targ_seq_len, targ_seq_len)\n",
    "    scaled_scores = tf.matmul(query, key, transpose_b=True) / np.sqrt(key_dim)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
    "\n",
    "    softmax = tf.keras.layers.Softmax()\n",
    "    weights = softmax(scaled_scores) \n",
    "    # output of scaled_dot_product_attention\n",
    "    output = tf.matmul(weights, value)\n",
    "    \n",
    "    output = block.mha2.merge_heads(output) # output.shape = (batch_size, targ_seq_len, d_model)\n",
    "    output = block.mha2.dense(output) # output.shape = (batch_size, targ_seq_len, d_model)\n",
    "    mha_output2 = output\n",
    "    \n",
    "    mha_output2 = block.dropout2(mha_output2, training=training)\n",
    "    mha_output2 = block.layernorm2(mha_output2 + mha_output1) # mha_output2.shape = (batch_size, tar_seq_len, embed_dim)\n",
    "\n",
    "    ffn_output = block.ffn(mha_output2)\n",
    "    ffn_output = block.dropout3(ffn_output, training=training)\n",
    "    output = block.layernorm3(ffn_output + mha_output2) # output.shape = (batch_size, seq_len, embed_dim)\n",
    "\n",
    "    target = output\n",
    "\n",
    "decoder_output = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27dcdb17-f496-4d09-a007-0fb72d4095a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 20])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a292047b-ebc7-4e3b-a048-4b0ee8b4d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_output = tfm.output_layer(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63ab0ad5-9c5f-4631-884a-b914c8540c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 40])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d065e6-0595-4682-a269-587c0cbc9e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f577727-bc78-42d3-8c03-08272e3c0c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63342f3a-dd11-4be1-aa3f-8a71059bcdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d69771-39d9-4a49-ac5f-be9083cf913f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e20a0d-107e-469a-90d1-7915b2ce63cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d505f0-4d7e-4de1-9e99-4002234e6bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4acc9-8746-454a-8b93-91d24bd802b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eca72d-5556-4007-8414-a8880c58f7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6a123-ea44-4214-9f15-a5c6e8febbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5602754c-88c9-46aa-b915-60ea74513a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2eae6-e26f-458b-a67e-e7575d99f9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b6bfe-2c27-4fba-bde3-34bd2f681697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e746cfa-94b8-41ef-a12c-4bfcf145a982",
   "metadata": {},
   "source": [
    "## multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf88d18-52d8-4349-8edb-e9e041cb7bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        assert(d_model % num_heads == 0)\n",
    "\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        # Linear layer to generate the final output.\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "  \n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        split_inputs = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_head))\n",
    "        return tf.transpose(split_inputs, perm=[0, 2, 1, 3])\n",
    "  \n",
    "    def merge_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        merged_inputs = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        return tf.reshape(merged_inputs, (batch_size, -1, self.d_model))\n",
    "\n",
    "    def call(self, query, key, value, mask = None):\n",
    "        # query, key and value have shape (batch_size, seq_len, embed_dim)\n",
    "        # mask has shape (batch_size, 1, 1, seq_len)\n",
    "        # if mask has shape (batch_size, seq_len) we can change it to proper shape by writing\n",
    "        # mask = mask[:, tf.newaxis, tf.newaxis, :]\n",
    "        \n",
    "        qs = self.wq(query) # qs, ks and vs have shape (batch_size, seq_len, d_model)\n",
    "        ks = self.wk(key)\n",
    "        vs = self.wv(value)\n",
    "\n",
    "        qs = self.split_heads(qs) # qs, ks and vs have shape (batch_size, num_heads, seq_len, d_head)\n",
    "        ks = self.split_heads(ks)\n",
    "        vs = self.split_heads(vs)\n",
    "\n",
    "        output, attn_weights = self.scaled_dot_product_attention(qs, ks, vs, mask)\n",
    "        output = self.merge_heads(output) # output.shape = (batch_size, seq_len, d_model)\n",
    "        output = self.dense(output) # output.shape = (batch_size, seq_len, d_model)\n",
    "        \n",
    "        return output, attn_weights\n",
    "    \n",
    "    def scaled_dot_product_attention(self, query, key, value, mask=None):\n",
    "        # query, key and value have shape (batch_size, num_heads, seq_len, d_head)\n",
    "        # mask has shape (batch_size, 1, 1, seq_len)\n",
    "        \n",
    "        key_dim = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        # scaled_scores has shape (batch_size, num_heads, seq_len, seq_len)\n",
    "        scaled_scores = tf.matmul(query, key, transpose_b=True) / np.sqrt(key_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
    "\n",
    "        softmax = tf.keras.layers.Softmax()\n",
    "        weights = softmax(scaled_scores) \n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99344cf0-b800-4843-95ce-85e9b5969277",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[1,2,0], [1,0,0]])\n",
    "query = key = value = x[:,:,tf.newaxis]\n",
    "mask = tf.cast(tf.math.not_equal(x, 0), tf.float32) # mask.shape = (batch_size, seq_len)\n",
    "mask = mask[:, tf.newaxis, tf.newaxis, :]\n",
    "mha = MultiHeadAttention(20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6df9dec-d6a5-43df-aa4b-f96d1b142622",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, _ = mha(query, key, value, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23276ae7-7505-40bf-8d29-d5ec148095ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 20])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163a7d1-c43f-4254-aff7-61a1a036e06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faade368-6bf4-438e-82c9-bb118b4ff4a0",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4451922-44b2-42b5-ac07-0a9926f9ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = self.feed_forward_network(d_model, hidden_dim)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "    def call(self, inputs, training, mask = None):\n",
    "        # x.shape = (batch_size, seq_len, embed_dim)\n",
    "        mha_output, attn_weights = self.mha(inputs, inputs, inputs, mask)\n",
    "        mha_output = self.dropout1(mha_output, training=training)\n",
    "        mha_output = self.layernorm1(inputs + mha_output)\n",
    "\n",
    "        ffn_output = self.ffn(mha_output)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        output = self.layernorm2(mha_output + ffn_output)\n",
    "\n",
    "        return output, attn_weights\n",
    "    \n",
    "    def feed_forward_network(self, d_model, hidden_dim):\n",
    "        return tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(hidden_dim, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860c4184-3f4c-4b0b-88e6-af27f009cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_blocks, d_model, num_heads, hidden_dim, src_vocab_size,\n",
    "               max_seq_len, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_encoding = self.positional_encoding(max_seq_len, d_model)\n",
    "\n",
    "        # The original Attention Is All You Need paper applied dropout to the\n",
    "        # input before feeding it to the first encoder block.\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        # Create encoder blocks.\n",
    "        self.blocks = [\n",
    "            EncoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate)\n",
    "            for _ in range(num_blocks)\n",
    "        ]\n",
    "  \n",
    "    def call(self, inputs, training, mask = None):\n",
    "        # inputs.shape = (batch_size, seq_len)\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        inputs = self.embedding(inputs) \n",
    "        inputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        inputs += self.pos_encoding[:, :seq_len, :]\n",
    "        inputs = self.dropout(inputs, training=training) # inputs.shape = (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            inputs, weights = block(inputs, training, mask = mask)\n",
    "\n",
    "        return inputs, weights\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                     np.arange(d_model)[np.newaxis, :],\n",
    "                                     d_model)\n",
    "        sines = np.sin(angle_rads[:, 0::2])\n",
    "        cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def get_angles(self, positions, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return positions * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7316c1c3-ce80-49b3-9a6c-752cdfe89a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x represents 2 sentences\n",
    "x = tf.constant([[1, 2, 0], [1, 0, 0]]) # x.shape = (batch_size, seq_len)\n",
    "mask = tf.cast(tf.math.not_equal(x, 0), tf.float32) # mask.shape = (batch_size, seq_len)\n",
    "mask = mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "encoder = Encoder(\n",
    "    num_blocks = 2, \n",
    "    d_model = 20, \n",
    "    num_heads = 5, \n",
    "    hidden_dim = 30, \n",
    "    src_vocab_size = 10,\n",
    "    max_seq_len = 3\n",
    ")\n",
    "output, weights = encoder(x, False, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "babec696-8ec7-477a-9e23-9a1627bf7989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 20])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ee29d-00b0-4641-b2b3-7d753c20e77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749f765-b5ed-481f-887a-4dff43167cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fb219-010d-4ffc-a49c-540e5c39bf30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c81fd52d-d5dd-4701-b399-652c0d252188",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34114e2-1dcc-45e0-ba43-32df3e4f7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = self.feed_forward_network(d_model, hidden_dim)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "    # Note the decoder block takes two masks. One for the first MHA, another\n",
    "    # for the second MHA.\n",
    "    def call(self, encoder_output, target, training, decoder_mask, encoder_decoder_mask):\n",
    "        # encoder_output.shape = (batch_size, inp_seq_len, embed_dim)\n",
    "        # target.shape = (batch_size, tar_seq_len, embed_dim)\n",
    "        # decoder_mask.shape = encoder_decoder_mask.shape = (batch_size, 1, 1, tar_seq_len)\n",
    "        # encoder_decoder_mask.shape = encoder_decoder_mask.shape = (batch_size, 1, 1, inp_seq_len)\n",
    "        \n",
    "        mha_output1, attn_weights = self.mha1(target, target, target, decoder_mask)\n",
    "        mha_output1 = self.dropout1(mha_output1, training=training)\n",
    "        mha_output1 = self.layernorm1(mha_output1 + target) # mha_output1.shape = (batch_size, tar_seq_len, embed_dim)\n",
    "\n",
    "        mha_output2, attn_weights = self.mha2(mha_output1, encoder_output, encoder_output, encoder_decoder_mask)\n",
    "        mha_output2 = self.dropout2(mha_output2, training=training)\n",
    "        mha_output2 = self.layernorm2(mha_output2 + mha_output1) # mha_output2.shape = (batch_size, tar_seq_len, embed_dim)\n",
    "\n",
    "        ffn_output = self.ffn(mha_output2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        output = self.layernorm3(ffn_output + mha_output2) # output.shape = (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        return output, attn_weights\n",
    "    \n",
    "    def feed_forward_network(self, d_model, hidden_dim):\n",
    "        return tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(hidden_dim, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbb7379a-7687-4b74-94a3-c08be7b10713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
    "                   max_targ_seq_len, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = self.positional_encoding(max_targ_seq_len, d_model)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.blocks = [DecoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate) for _ in range(num_blocks)]\n",
    "    \n",
    "    def call(self, encoder_output, target, training, decoder_mask = None, encoder_decoder_mask = None):\n",
    "        # encoder_output.shape = (batch_size, inp_seq_len, embed_dim)\n",
    "        # target.shape = (batch_size, targ_seq_len)\n",
    "        # decoder_mask.shape = encoder_decoder_mask.shape = (batch_size, 1, 1, targ_seq_len)\n",
    "        seq_len = tf.shape(target)[1]\n",
    "        target = self.embedding(target)\n",
    "        target *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        target += self.pos_encoding[:, :seq_len, :]\n",
    "        target = self.dropout(target, training=training) # .shape = (batch_size, targ_seq_len, embed_dim)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            target, weights = block(encoder_output, target, training, decoder_mask, encoder_decoder_mask)\n",
    "\n",
    "        return target, weights # x.shape = (batch_size, targ_seq_len, embed_dim)\n",
    "    \n",
    "    def positional_encoding(self, max_length, d_model):\n",
    "        position = tf.range(max_length, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.pow(10000, (2 * tf.range(d_model // 2, dtype=tf.float32)) / d_model)\n",
    "        div_term = div_term[tf.newaxis, :]\n",
    "\n",
    "        # Compute the sine and cosine components\n",
    "        angles = tf.matmul(position, div_term)\n",
    "        pos_enc = tf.concat([tf.sin(angles), tf.cos(angles)], axis=-1)\n",
    "        \n",
    "        pos_enc = pos_enc[tf.newaxis, :]\n",
    "\n",
    "        return pos_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd46367-efc1-47e1-9fb9-0f473c1bc330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac2835-0a1a-4396-82eb-b7fdc8d6a7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "335c4e28-c1b9-4c38-ab30-b25fe3910526",
   "metadata": {},
   "source": [
    "## transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63833c8a-2696-4089-9651-a92f496c781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_blocks, d_model, num_heads, hidden_dim, source_vocab_size,\n",
    "                   target_vocab_size, max_input_len, max_target_len, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_blocks, d_model, num_heads, hidden_dim, source_vocab_size, \n",
    "                               max_input_len, dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
    "                               max_target_len, dropout_rate)\n",
    "\n",
    "        # The final dense layer to generate logits from the decoder output.\n",
    "        self.output_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, targets, training, encoder_mask = None,\n",
    "               decoder_mask = None, encoder_decoder_mask = None):\n",
    "        encoder_output, encoder_attn_weights = self.encoder(inputs, \n",
    "                                                            training, encoder_mask)\n",
    "\n",
    "        decoder_output, decoder_attn_weights = self.decoder(encoder_output, \n",
    "                                                            targets, training,\n",
    "                                                            decoder_mask, encoder_decoder_mask)\n",
    "        \n",
    "        output = self.output_layer(decoder_output) # output.shape = (batch_size, seq_len, target_vocab_size)\n",
    "        \n",
    "        return output, encoder_attn_weights, decoder_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49f0c25f-a55f-4187-9aee-a45dd7af1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.constant(np.random.rand(2, 2))\n",
    "target = tf.constant(np.random.rand(2, 3))\n",
    "encoder_mask = tf.cast(tf.math.not_equal(inputs, 0), tf.float32)\n",
    "decoder_mask = tf.linalg.band_part(tf.ones((tf.shape(target)[0], tf.shape(target)[1])), -1, 0)\n",
    "encoder_decoder_mask = tf.cast(tf.math.not_equal(inputs, 0), tf.float32)\n",
    "\n",
    "encoder_mask = encoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "encoder_decoder_mask = encoder_decoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "decoder_mask = decoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_blocks = 5,\n",
    "    d_model = 20,\n",
    "    num_heads = 5,\n",
    "    hidden_dim = 30,\n",
    "    source_vocab_size = 40,\n",
    "    target_vocab_size = 40,\n",
    "    max_input_len = inputs.shape[1],\n",
    "    max_target_len = target.shape[1]\n",
    ")\n",
    "\n",
    "output, _, _ = transformer(inputs, target, True, encoder_mask, decoder_mask, encoder_decoder_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cccad947-5e5a-41d0-a328-90b7c8d57ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 40])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ee287-1836-4d5e-a909-9b5a403b2094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cabf24-5f63-4077-af8b-d43d3e920acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51cc8a4f-b727-43da-bce1-9521745a1eff",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f2cac-7012-4150-8acf-b90eb9896621",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.constant([[[0., 1.], [0., 1.]], [[0., 1.], [1., 0.]]])\n",
    "targets = tf.constant([[1,1], [1,0]])\n",
    "loss_function(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39998488-37f1-47df-ac62-65295ab0450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.constant([[0., 1.], [1., 0.]])\n",
    "targets = tf.constant([1, 0])\n",
    "loss_function(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "245b447b-5716-4fb9-bd8c-0db00627dbff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.1697 Accuracy 0.0000\n",
      "Epoch 1 Batch 0 Loss 7.8966 Accuracy 0.3818\n",
      "Epoch 1 Batch 0 Loss 7.7377 Accuracy 0.5058\n",
      "Epoch 1 Batch 0 Loss 7.5915 Accuracy 0.5483\n",
      "Epoch 1 Batch 0 Loss 7.4422 Accuracy 0.5709\n",
      "Epoch 2 Batch 0 Loss 6.6324 Accuracy 0.6347\n",
      "Epoch 2 Batch 0 Loss 6.4889 Accuracy 0.6353\n",
      "Epoch 2 Batch 0 Loss 6.3185 Accuracy 0.6386\n",
      "Epoch 2 Batch 0 Loss 6.1489 Accuracy 0.6382\n",
      "Epoch 2 Batch 0 Loss 5.9737 Accuracy 0.6389\n",
      "Epoch 3 Batch 0 Loss 5.0490 Accuracy 0.6347\n",
      "Epoch 3 Batch 0 Loss 4.9094 Accuracy 0.6353\n",
      "Epoch 3 Batch 0 Loss 4.7314 Accuracy 0.6386\n",
      "Epoch 3 Batch 0 Loss 4.5672 Accuracy 0.6382\n",
      "Epoch 3 Batch 0 Loss 4.4019 Accuracy 0.6389\n",
      "Epoch 4 Batch 0 Loss 3.5771 Accuracy 0.6347\n",
      "Epoch 4 Batch 0 Loss 3.4898 Accuracy 0.6353\n",
      "Epoch 4 Batch 0 Loss 3.3693 Accuracy 0.6386\n",
      "Epoch 4 Batch 0 Loss 3.2916 Accuracy 0.6382\n",
      "Epoch 4 Batch 0 Loss 3.2270 Accuracy 0.6389\n",
      "Epoch 5 Batch 0 Loss 2.9886 Accuracy 0.6347\n",
      "Epoch 5 Batch 0 Loss 3.0109 Accuracy 0.6353\n",
      "Epoch 5 Batch 0 Loss 2.9793 Accuracy 0.6386\n",
      "Epoch 5 Batch 0 Loss 2.9773 Accuracy 0.6382\n",
      "Epoch 5 Batch 0 Loss 2.9702 Accuracy 0.6389\n",
      "Epoch 6 Batch 0 Loss 2.9482 Accuracy 0.6347\n",
      "Epoch 6 Batch 0 Loss 2.9844 Accuracy 0.6353\n",
      "Epoch 6 Batch 0 Loss 2.9571 Accuracy 0.6386\n",
      "Epoch 6 Batch 0 Loss 2.9586 Accuracy 0.6382\n",
      "Epoch 6 Batch 0 Loss 2.9538 Accuracy 0.6389\n",
      "Epoch 7 Batch 0 Loss 2.9399 Accuracy 0.6347\n",
      "Epoch 7 Batch 0 Loss 2.9650 Accuracy 0.6353\n",
      "Epoch 7 Batch 0 Loss 2.8839 Accuracy 0.6386\n",
      "Epoch 7 Batch 0 Loss 2.8247 Accuracy 0.6382\n",
      "Epoch 7 Batch 0 Loss 2.7706 Accuracy 0.6389\n",
      "Epoch 8 Batch 0 Loss 2.5443 Accuracy 0.6347\n",
      "Epoch 8 Batch 0 Loss 2.5547 Accuracy 0.6353\n",
      "Epoch 8 Batch 0 Loss 2.5103 Accuracy 0.6386\n",
      "Epoch 8 Batch 0 Loss 2.4946 Accuracy 0.6382\n",
      "Epoch 8 Batch 0 Loss 2.4749 Accuracy 0.6390\n",
      "Epoch 9 Batch 0 Loss 2.3917 Accuracy 0.6395\n",
      "Epoch 9 Batch 0 Loss 2.4229 Accuracy 0.6433\n",
      "Epoch 9 Batch 0 Loss 2.3911 Accuracy 0.6508\n",
      "Epoch 9 Batch 0 Loss 2.3864 Accuracy 0.6522\n",
      "Epoch 9 Batch 0 Loss 2.3776 Accuracy 0.6543\n",
      "Epoch 10 Batch 0 Loss 2.3417 Accuracy 0.6540\n",
      "Epoch 10 Batch 0 Loss 2.3778 Accuracy 0.6556\n",
      "Epoch 10 Batch 0 Loss 2.3492 Accuracy 0.6599\n",
      "Epoch 10 Batch 0 Loss 2.3477 Accuracy 0.6592\n",
      "Epoch 10 Batch 0 Loss 2.3411 Accuracy 0.6600\n"
     ]
    }
   ],
   "source": [
    "def train_step(inputs, targets, transformer, optimizer, loss_function, train_loss, train_accuracy):\n",
    "    # Initialize the mask variables\n",
    "    encoder_mask, encoder_decoder_mask, decoder_mask = create_masks(inputs, targets)\n",
    "    # encoder_mask = decoder_mask = encoder_decoder_mask = None\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Pass the inputs through the transformer\n",
    "        # last word in targets is <end> token\n",
    "        predictions, _, _ = transformer(inputs, targets[:, :-1], True, encoder_mask, decoder_mask, encoder_decoder_mask)\n",
    "      \n",
    "        # Calculate the loss\n",
    "        # first word in targets is the <start> token\n",
    "        loss = loss_function(targets[:, 1:], predictions)\n",
    "    \n",
    "    # Apply the gradients\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    # Update the loss and accuracy\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets[:, 1:], predictions)\n",
    "\n",
    "# Function to create masks\n",
    "def create_masks(inputs, targets):\n",
    "    encoder_mask = tf.cast(tf.math.not_equal(inputs, 0), tf.float32)\n",
    "    decoder_mask = tf.linalg.band_part(tf.ones((tf.shape(targets[:, :-1])[0], tf.shape(targets[:, :-1])[1])), -1, 0)\n",
    "    encoder_decoder_mask = tf.cast(tf.math.not_equal(inputs, 0), tf.float32)\n",
    "    \n",
    "    encoder_mask = encoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "    encoder_decoder_mask = encoder_decoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "    decoder_mask = decoder_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "    # all masks have shape = (batch_size, 1, 1, seq_len)\n",
    "  \n",
    "    return encoder_mask, encoder_decoder_mask, decoder_mask\n",
    "\n",
    "# Initialize the Transformer model, optimizer, and loss function\n",
    "transformer = Transformer(\n",
    "    num_blocks = 5,\n",
    "    d_model = 20,\n",
    "    num_heads = 5,\n",
    "    hidden_dim = 30,\n",
    "    source_vocab_size = enc_vocab_size,\n",
    "    target_vocab_size = dec_vocab_size,\n",
    "    max_input_len = enc_seq_length,\n",
    "    max_target_len = dec_seq_length\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Initialize the metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 20\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    # Iterate over the training dataset\n",
    "    for batch_number in range(len(trainX) // batch_size + 1):\n",
    "        inputs = trainX[batch_number * batch_size : (batch_number + 1) * batch_size]\n",
    "        targets = trainY[batch_number * batch_size : (batch_number + 1) * batch_size]\n",
    "        train_step(inputs, targets, transformer, optimizer, loss_function, train_loss, train_accuracy)\n",
    "      \n",
    "        # Print training progress every few batches\n",
    "        if batch_number % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch_number} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "28d77650-321c-4cc1-9a24-7ed17369ce8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['the', 'end', 'start', 'of', 'in', 'and', 'to', 'a', 'he', 'observatory', 'was', 'that', 'is', 'at', 'for', 'telescope', 'his', 'first', 'canada', 'it', 'as', 'astronomy', 'astronomers', 'from', 'stars', 'by', 'university', 'an', 'with', 'observatories', 'its', 'radio', 'on', 'canadian', 'space', 'research', 'became', 'which', 'world', 'new', 'are', 'years', 'astronomical', 'work', 'toronto', 'ã‚â«', 'return', 'this', 'light', 'be', 'instruments', 'born', 'one', 'study', 'two', 'national', 'dominion', 'time', 'british', 'degree', 'were', 'also', 'ã¢â‚¬â€œ', 'astrophysical', 'received', 'universe', '1', 'our', 'evolution', 'ontario', 'large', 'columbia', 'department', 'used', 'their', 'star', 'would', 'about', 'year', 'astrophysics', 'during', 'institute', 'largest', 'established', 'images', 'quebec', 'mont', 'mãƒâ©gantic', 'began', 'mirror', 'physics', 'director', 'metre', 'most', 'project', 'several', 'society', 'many', 'other', 'david', 'professor', 'made', 'these', 'telescopes', 'metres', 'where', 'later', 'canadaã¢â‚¬â„¢s', 'solar', 'galaxy', 'astronomes', 'sky', 'victoria', 'she', 'astronomer', 'only', 'has', 'united', 'dunlap', 'age', 'john', 'council', 'named', 'into', 'chant', 'king', 'radiation', 'published', 'interstellar', 'france', 'after', 'near', 'or', 'sun', 'will', 'galaxies', 'using', 'very', 'her', 'doctoral', 'international', 'hawaii', 'known', 'than', 'when', 'royal', 'position', 'not', 'obtained', 'cosmic', 'between', 'james', 'bachelorã¢â‚¬â„¢s', 'north', 'but', 'more', 'states', 'plaskett', 'centre', 'herzberg', 'discovery', 'liquid', 'until', 'moon', '3', 'data', 'numerous', 'type', 'matter', 'neutrinos', 'have', 'satellite', 'ottawa', 'wavelengths', 'american', 'science', 'all', 'high', 'had', 'algonquin', 'next', 'second', 'size', 'same', 'way', 'including', 'planets', 'so', 'such', 'clusters', 'inaugurated', 'around', '1972', 'black', 'idea', '2', 'white', 'took', 'gemini', 'then', 'they', 'member', 'centimetre', 'installed', 'college', 'still', 'today', 'designed', 'because', 'structure', 'each', 'system', 'surface', 'up', 'bolton', 'goal', 'water', 'measuring', 'part', 'infrared', 'studies', 'major', 'determine', '1977', 'equipped', 'beals', 'objects', 'montreal', 'construction', '1987', 'what', 'array', 'scientific', 'racine', 'can', 'thus', 'atmosphere', 'sunã¢â‚¬â„¢s', 'fontaine', 'comets', 'order', '1971', '000', 'earth', 'hubble', 'william', 'frederick', 'created', 'background', 'bond', 'renãƒâ©', 'make', '10', 'hired', 'like', 'clouds', 'masterã¢â‚¬â„¢s', 'who', 'over', 'reeves', 'mckellar', 'honour', 'could', 'awards', '1970', 'helen', 'observations', 'discovered', 'theoretical', 'energy', 'died', 'hole', 'through', 'martin', 'milky', 'operate', 'launched', 'ever', 'centimetres', 'matthews', 'covington', 'diameter', 'since', 'role', 'example', '5', 'completed', '1962', '1960', '1999', 'measure', 'van', 'quickly', 'creation', 'field', 'park', 'observation', 'temperature', 'chile', 'provide', 'found', 'pollution', 'returned', 'emitted', 'studying', 'stellar', 'mass', 'wrote', 'expert', 'earthã¢â‚¬â„¢s', 'variations', 'small', 'video', 'government', 'him', 'astrolab', 'demonstrated', 'laval', '6', 'day', 'number', 'charles', 'gas', 'de', 'visible', 'ultraviolet', 'hutchings', '1997', 'name', 'public', 'celestial', 'kilometres', 'mission', 'antennas', '1991', 'possible', 'life', 'underhill', 'three', 'them', 'rothney', 'another', 'consists', 'chemical', 'site', '1978', 'amateur', 'parabolic', 'den', 'articles', 'ob', 'across', 'french', 'origin', 'involved', 'being', 'left', 'long', 'before', 'elected', 'there', 'interest', 'book', 'hogg', 'helped', 'related', 'decided', 'point', 'existence', 'original', 'penticton', 'orion', 'spectroscopy', 'spectrum', 'observe', 'south', 'molecular', 'thomas', 'state', 'mainly', 'observatoryã¢â‚¬â„¢s', 'become', 'submillimetre', 'activity', '1973', 'researcher', 'fuse', 'times', 'shape', 'activities', 'tremaine', '2001', 'city', '1969', 'scientist', 'molecules', 'use', 'camera', 'detect', 'formation', 'postdoctoral', 'joined', 'peter', 'participation', 'program', 'image', 'newton', 'cambridge', 'carlyle', 'specialist', 'elements', 'vancouver', 'atacama', 'information', 'composition', 'again', 'sawyer', 'due', 'binary', 'sunspots', 'flux', 'built', 'explains', 'saint', 'evidence', 'ã‚â©', '2006', 'realization', '1968', 'giant', 'nebula', 'responsible', 'spectra', 'takes', 'own', 'dwarf', '1979', 'moved', 'tonnes', 'responsibility', '1994', 'cosmology', 'webb', 'roy', 'generation', 'clerk', 'maxwell', 'nova', 'every', 'four', 'advanced', 'confirmed', 'less', 'once', '1981', 'massive', 'been', 'waves', 'technology', 'some', 'optical', 'america', 'range', 'predicted', '60', 'photographic', '1995', 'map', 'precision', 'speed', 'those', 'days', 'main', '1928', 'smith', 'take', 'x', 'scale', 'inauguration', 'instrument', 'researchers', 'located', '1996', 'great', 'vibrations', '1990', 'president', 'bergh', 'london', 'thanks', 'books', 'reference', 'cold', 'rays', 'photograph', 'now', 'jean', '1974', 'operating', 'develop', 'lines', '15', 'quasars', 'identified', 'planet', 'makes', 'design', 'regions', 'wavelength', 'heavy', '1980ã¢â‚¬â„¢s', 'edwin', 'constructed', 'enrolled', 'following', 'much', 'distinctions', 'under', '1935', 'joseph', '2000', 'nuclear', 'various', 'old', 'played', 'early', '1905', '83', 'asteroid', 'understand', 'mathematical', 'oldest', '1948', 'stayed', 'atlas', 'galactic', 'degrees', 'resolution', 'determined', 'post', 'access', 'explorers', 'allow', '1940', 'service', '8', 'antenna', 'globular', 'massachusetts', 'hot', 'atmospheres', 'home', 'lake', 'lenses', 'andrew', 'best', 'projects', 'operational', 'provided', 'millimeter', 'stanley', 'detected', 'big', 'ã¢â€”â¦', 'managed', 'etc', 'guide', 'variable', 'however', 'presence', '1946', 'any', 'sequence', 'river', 'sidney', 'measurement', 'invents', 'caused', 'hill', '46', 'distance', 'interested', 'career', '1904', 'people', 'among', 'historical', 'electromagnetic', 'proposal', 'officer', 'given', 'neutrino', 'worked', 'building', 'asteroids', 'england', 'retirement', 'hemisphere', 'opened', 'dome', '14', 'equipment', '2003', 'fact', 'retired', '1976', 'marked', 'results', 'george', 'allows', 'studied', 'mark', 'hydrogen', 'scotia', 'mirrors', 'arthur', 'collected', 'positions', 'full', 'we', 'passed', '1966', 'richard', 'magnetic', '25', 'body', 'task', 'specific', 'radial', 'away', 'proposed', 'mauna', 'kea', 'gilles', 'asteroseismologist', 'photographs', 'amount', 'complete', 'see', 'reactions', 'orbit', 'odin', 'specializes', 'oxygen', 'no', 'taught', 'effort', 'dust', 'jaymie', '1950', 'imager', 'active', 'war', 'survey', 'obtain', '1892', 'lecturer', 'alexander', 'cross', 'signed', 'making', 'started', 'taken', 'ccd', 'ii', 'prize', '1988', '1989', '1965', 'collect', 'currently', '22', 'kingdom', '18', '4', 'powerful', 'months', 'six', 'able', '400', 'popularizer', '1910', 'continued', 'measured', 'europe', 'shortly', 'bang', 'itself', 'luminosity', 'measures', 'intensity', 'focused', 'total', 'popularize', 'era', 'jesuits', 'contributions', 'adaptive', 'optics', 'ten', 'dark', 'spots', 'official', '1956', 'richmond', 'place', 'history', 'precise', 'simulate', 'knowledge', 'seen', 'gordon', '1982', 'demonstrate', 'clarence', 'augustus', 'test', 'photography', 'sudbury', 'below', 'succeeded', 'ultra', 'colleagues', 'lithium', 'interior', 'result', 'collaboration', 'within', 'discover', 'detection', 'team', 'almost', 'beginning', 'sources', 'art', 'observed', 'popular', 'highest', 'while', 'detector', 'rotation', 'form', 'short', 'pass', 'operated', '1984', 'carbon', 'drawing', 'facility', '1922', 'mount', 'purpose', 'despite', 'seven', 'launch', 'entirely', 'lemay', 'particles', 'becomes', 'students', 'courses', 'specifically', 'general', 'gerhard', 'robert', 'constructing', '0', 'millimetres', 'construct', 'velocities', 'moving', 'observer', 'dynamics', 'housed', 'force', 'volcano', 'geophysics', 'sensitive', 'shake', 'inventor', 'particularly', 'facilities', 'record', '88', 'lunar', 'open', 'federal', 'western', 'mapping', 'elevation', 'set', 'enormous', 'forces', 'las', 'campanas', 'key', 'wolf', 'rayet', 'compounds', 'upon', 'distribution', 'well', 'frank', 'biggest', 'network', 'pulsations', 'accepted', 'young', 'weighs', 'kilograms', 'collaborative', '1865', '1941', 'charge', 'scott', 'duncan', 'follow', 'finance', 'approximately', 'five', 'jack', 'existed', 'fortress', 'california', '50', '1929', '1943', '1967', 'third', 'spent', 'calculations', 'kuiper', 'include', 'permanent', 'plane', 'portion', 'galaxial', 'different', 'discoveries', 'medal', 'award', 'interests', 'both', 'primarily', 'blue', 'thick', 'traverse', 'netherlands', 'teaching', 'transferred', 'street', '11', 'development', 'uses', 'million', 'likely', '500', 'hubert', '1932', 'talented', 'often', 'understanding', 'photometry', 'internal', 'cameras', 'turbulence', 'problems', 'eclipse', 'newly', 'surrounded', 'classification', 'distances', 'lawrence', 'terrestrial', 'atmospheric', 'kingston', 'important', 'greatly', 'modern', 'previous', 'may', 'physical', 'low', 'honorary', 'paris', 'considered', '2005', 'quãƒâ©bec', 'father', 'ã¢â‚¬â¨of', 'models', 'create', 'underground', 'direct', 'unfortunately', 'cause', 'vessel', '7', 'along', 'beryllium', 'boron', '1992', 'demonstrating', 'supernova', 'us', 'principal', 'division', 'chief', 'fields', 'disciplines', 'planetary', 'pursued', 'successor', 'natural', 'chemist', 'specialized', '1916', 'stature', 'weighed', 'dwarfs', 'doors', 'lecture', 'holes', 'purchase', 'education', 'imaging', 'nitrogen', 'material', 'emission', 'emit', 'addition', 'electricity', 'emeritus', 'super', 'microwaves', 'impact', 'do', 'change', 'property', 'basic', 'model', 'required', 'move', 'atoms', 'subsequently', 'developed', 'cn', 'calgary', 'perfecting', 'european', 'need', 'leader', 'allowed', '1986', 'television', 'school', 'calculation', 'working', 'mandate', 'perfect', 'your', 'went', '1923', 'microwave', 'contributed', 'laboratory', 'colossal', 'detecting', '1964', 'coated', 'probe', 'focuses', 'eastern', 'job', 'levels', 'venture', 'agreement', '1985', 'offered', 'added', 'larger', 'last', 'exact', 'headquarters', 'exceptional', 'spectral', 'cyanogen', 'ch', 'establish', '1980', '21', 'corresponds', 'close', 'ancient', 'birth', '2002', 'structures', 'distant', 'arrival', 'professorship', 'know', 'little', 'attached', '12', 'spectroscope', 'concentrated', 'deuterium', 'processes', 'northern', 'collaborators', '1939', 'absorbs', 'communications', 'production', 'dr', '1930', 'interference', 'land', 'rest', 'country', 'photometer', '1958', 'collecting', 'excellence', 'transit', 'outside', 'bears', 'gave', 'outstanding', 'entire', 'hickson', 'successful', '1947', 'even', 'paul', 'earned', 'predictions', 'ozone', 'better', 'renowned', 'centres', 'partnership', 'chairman', 'j', 'realized', 'barrie', 'explorer', 'museum', 'jacques', 'environment', 'germany', 'theory', '1900', 'precisely', '1913', 'changed', 'founded', '1942', 'desert', '20', 'forced', 'systems', 'measurements', 'region', 'night', 'satellites', 'certainty', 'current', 'alberta', 'gravitational', 'trillion', 'deep', 'cosmological', 'uranus', 'wilson', 'german', 'moons', '1899', 'famous', 'photos', 'out', 'introduction', 'host', 'death', 'companion', 'free', 'complex', 'conduct', 'expectancy', 'dutch', 'gerard', 'class', 'guidance', 'achievement', 'pacific', '38', 'capable', 'meteors', 'already', '1931', '1959', 'community', 'closed', 'seminary', '1960ã¢â‚¬â„¢s', 'actively', 'participated', 'productive', 'occupied', 'megaprime', 'probably', 'highly', 'called', 'poet', 'present', 'parameters', 'basis', 'improving', '1926', 'professional', 'planned', 'japan', 'largely', 'celebrated', 'baptized', 'pattern', 'bonnãƒâ©camps', '1759', 'nrc', 'workshop', 'object', 'extract', 'scientists', 'eclipses', 'philip', 'teece', 'inspired', 'women', 'enter', 'corrects', '1983', 'unable', 'daily', 'advantage', 'monitor', 'cliff', 'asteroseismology', 'diploma', 'administration', '1970ã¢â‚¬â„¢s', 'red', 'estimates', 'meteor', 'types', 'theoretically', 'provider', 'astrometry', 'computers', 'conjunction', '26', '074', 'reveal', 'invisible', '1950ã¢â‚¬â„¢s', 'increasingly', 'temperatures', 'awarded', 'organization', 'thousands', 'native', 'navigate', 'suggests', 'without', 'century', '1918', 'borra', 'creating', 'sought', 'calculated', 'tour', 'depth', '1924', 'enclosed', 'pure', 'normal', 'wide', 'process', 'creates', 'anne', 'retains', 'others', 'article', 'rotational', 'step', 'surveys', '1886', 'orbits', 'analysis', 'forming', 'db', '1854', 'raised', 'combined', 'mobile', 'parts', 'exploration', 'southern', '28', 'millionths', 'mid', 'display', 'housing', '41', 'nebulas', 'glass', '35', 'promote', 'level', 'cassegrain', 'reflecting', 'anomalies', 'obn', 'neighbouring', 'cygnus', 'princeton', 'remains', 'cannot', 'experience', 'mechanics', 'titled', 'everyone', 'sextant', 'longitudes', 'latitudes', 'pursuing', 'normally', 'events', 'latitude', 'indeed', 'immediately', 'nonetheless', 'controls', 'fully', 'computerized', 'coupled', 'spend', 'computer', 'generate', 'enough', 'real', 'interactive', 'displays', 'multimedia', 'presentations', 'special', 'continuously', 'proof', 'cycle', 'thirty', 'constitutes', 'worldã¢â‚¬â„¢s', 'database', 'adjunct', 'ocean', 'question', 'hadley', 'down', 'engineer', 'sequenceã¢â‚¬â¨ã¢â‚¬â¨john', 'functioning', 'greater', 'forays', '1954', 'hypothesis', 'conducted', 'apollo', 'estimate', 'density', 'passing', 'met', 'prediction', 'summer', 'chemistry', 'motions', 'edison', 'company', 'electric', 'gained', 'spectacular', 'encouraged', '1990ã¢â‚¬â„¢s', 'geological', 'gravimetric', 'geophysical', 'constant', 'town', 'stewart', 'mechanical', 'nicãƒâ©phore', 'niãƒâ©pce', 'lower', '1993', 'spectroscopic', 'guest', 'toward', 'thousand', 'calibre', 'saanich', 'gravity', 'expansion', 'explosion', 'involving', 'above', 'atop', 'la', 'commission', 'expanded', 'consisted', '700', 'posts', 't', 'future', 'really', 'developing', 'soon', 'teams', 'continues', '1927', 'few', 'works', 'got', '1945', 'how', 'came', 'did', 'come', 'holds', 'plaskettã¢â‚¬â„¢s', 'optician', 'publishes', 'house', 'locations', 'longitude', 'points', 'components', 'weigh', 'notably', 'included', 'relations', 'cygni', 'gaseous', 'surfaces', 'available', 'plains', 'abraham', 'cfwt', 'organic', 'methyne', 'difficult', 'corporation', 'acquired', 'instead', 'share', 'effectively', 'date', 'amounts', 'invent', 'track', 'summit', 'although', 'investigator', 'transparent', 'destroy', 'longer', 'vapour', 'head', 'eastman', 'builds', 'toldervy', 'fredericton', 'brunswick', 'garden', 'houses', 'assistant', 'darmstadt', 'herschel', 'manmade', '1911', 'surveyor', 'saw', 'sweden', 'countries', 'show', 'picture', 'ã‚â°c', '1750', 'crampton', 'suitcase', '4230', 'obtaining', 'doyon', 'humans', 'relationship', 'gathering', 'mathematics', 'refracting', 'retained', '600', 'laid', 'foundation', 'astrophotography', 'periods', '200', 'clock', 'buildings', 'clocks', 'experiment', 'repaired', 'operates', 'elsewhere', 'boltwood', 'having', 'virtually', 'maple', 'ridge', 'newfoundland', 'cabot', 'foot', 'soil', 'layer', 'output', 'necessary', 'certain', '62', 'represents', 'cosmos', 'aeronomic', 'help', 'spatial', 'coordinates', '1907', 'board', 'f', 'stands', 'far', 'just', 'directors', 'nasa', 'planning', 'resurface', 'assigned', 'finally', 'continental', 'distributed', 'saskatchewan', 'gore', 'opening', 'hst', 'rotates', 'status', 'rotating', 'publishing', 'changes', 'terms', 'chabert', 'owned', 'temporary', 'crater', 'berkeley', '24', 'put', 'llano', 'chajnantor', 'plateau', 'efforts', 'bore', 'fruit', 'b', 'sketch', 'shows', 'agent', 'radar', 'speeds', 'causes', 'affect', '1921', 'lawyer', 'mining', 'entrepreneur', 'suggested', 'subclass', 'single', 'throughout', 'establishment', '1850', 'base', 'ceased', 'cartier', 'castle', 'estate', 'desbarres', 'falmouth', 'saskatoon', 'astrophysicist', 'owns', '250', 'together', '1975', '75', 'search', 'product', 's', 'recognition', 'replace', 'lies', 'engraving', 'louisbourg', 'expanding', 'themselves', 'nobel', 'theories', 'physicist', 'yet', 'feedback', 'changing', 'credits', 'atomic', 'maintained', 'astronauts', 'johannes', 'geiss', 'collisions', 'topic', 'crucial', 'implications', 'human', 'power', 'elementary', 'schmidt', 'pair', 'coverage', 'produce', '97', 'patience', 'finished', 'argentina', 'anywhere', 'according', 'boomerang', 'interfere', 'gãƒâ¶ttingen', 'einstein', '1963', 'occur', '1908', 'palomar', '1933', 'significant', 'nothing', 'whereas', 'port', 'remained', 'skies', '32', 'st', 'confirm', 'headlines', '1951', 'champlain', 'means', 'operation', 'observing', 'rings', 'zenith', 'performing', 'personal', 'amusement', 'extrasolar', 'detailed', 'lying', 'edges', 'copenhagen', 'denmark', 'good', 'committee', 'manages', 'aims', 'common', 'meant', 'valuable', 'navigates', 'donation', 'contribution', 'fine', 'sensors', 'aimed', 'targets', 'renovation', 'photo', 'compositions', 'queen', 'elizabeth', 'silver', 'jubilee', 'ken', 'chilton', 'providing', 'berghã¢â‚¬â„¢s', 'specialty', 'diversified', 'supernovae', 'radcliffe', 'wanted', 'advance', '49', 'tasks', 'bought', 'tasco', 'reflector', 'joining', 'megacam', '340', 'pixel', 'pixels', 'assyrians', 'magnify', 'bc', 'estimating', 'sole', 'train', 'glossary', 'installation', 'movement', 'urban', 'plagued', 'problem', 'constituents', 'fall', 'hands', 'never', 'institutions', 'reorganized', 'responsibilities', 'technique', 'meanwhile', 'absence', 'suitable', 'edition', 'links', 'necessarily', 'spectrometers', 'distortion', 'whether', 'partial', 'november', '23', 'covered', 'wooden', 'ã¢â‚¬â¨ã¢â‚¬â¨gilles', 'poorly', 'understood', 'business', 'manager', 'stores', 'sears', 'marks', 'spencer', 'finds', 'mouth', 'perseid', 'shower', 'horace', 'welcome', 'babcock', 'procedure', 'distortions', 'est', 'eventually', 'carved', 'enviable', 'scene', '91', 'guaranteed', 'irrefutable', 'facilitated', 'mention', 'chamber', 'archives', '1770', 'kilometre', 'separating', 'penetrate', 'details', 'otherwise', '000ã‚â°', 'c', 'broadened', 'passage', 'publications', 'written', 'specialists', 'modify', 'cobourg', '1846', 'equal', '19th', 'onwards', 'atom', 'spurred', 'discipline', 'ermanno', 'franco', 'revived', 'feasibility', 'ordre', 'du', 'quite', 'triumph', 'bourdon', 'accomplish', 'numerical', 'simulation', 'weighted', 'cool', 'contribute', 'underlying', 'mechanisms', 'film', 'astro', 'visitors', 'reasons', '070', 'creighton', 'mine', 'associated', 'widow', 'jessie', 'sure', '7x50', 'binoculars', 'norton', 'pole', '35ã‚â°', 'lodged', 'immense', 'cavity', '34', 'equivalent', 'storey', 'spallation', 'barbara', 'conferred', 'diverse', 'subjects', 'refuted', 'significance', 'nazca', 'peru', 'unlikely', 'explosions', 'extinctions', 'prompts', 'ask', 'deepest', 'existential', 'questions', 'afterwards', 'rose', 'echelons', 'nomination', 'inspector', 'whole', 'lie', 'domain', 'geologists', 'biologists', 'astrobiology', 'possibility', '1792', 'murdoch', 'lamp', 'cities', 'britain', 'begin', 'streets', 'wiveleslie', 'abney', 'predict', 'pulsating', 'stowmarket', 'suffolk', 'units', '42', 'systematic', 'thousandths', 'contrast', 'observes', 'specialties', 'adjacent', 'room', 'ray', 'lã¢â‚¬â„¢astronomie', 'et', 'son', 'histoire', 'tonne', 'disk', 'qualified', 'citizens', '92', 'laurent', 'transfer', 'nearby', 'codes', 'absorption', 'arises', 'appears', 'traps', 'falls', 'invaluable', 'belong', 'photographer', 'adams', 'whipple', 'daguerreotype', 'octant', 'predecessors', 'quality', 'producing', 'shield', 'impede', 'noteworthy', 'apparent', 'determination', 'function', '1930ã¢â‚¬â„¢s', 'sites', 'trip', 'putting', 'debate', 'right', 'course', 'useful', 'medicine', 'americans', 'willard', 'boyle', 'quarter', 'bonnet', 'interface', 'modeled', 'comparison', 'includes', 'planetarium', 'offers', 'introduce', 'walks', 'ideal', 'monitoring', 'particular', 'newtonã¢â‚¬â„¢s', 'sequenceã¢â‚¬â¨ã¢â‚¬â¨jaymie', 'advantages', 'extra', 'neutron', 'windsor', 'drawn', 'giovanni', 'batista', 'hodierna', 'waterways', 'amazement', 'beyond', 'holyoke', 'generous', 'benefactor', 'inspiration', 'legends', 'telecommunications', 'tãƒâ©lãƒâ©phone', 'telus', 'specially', 'setbacks', 'expected', 'predecessor', 'kitt', 'peak', 'experiments', 'missions', 'likewise', 'aligned', 'polarized', 'grains', 'scepticism', 'voyager', 'spacecraft', 'flew', 'past', 'conventional', 'cheaper', 'build', '100', 'expensive', 'constantly', 'sorts', 'popularizing', 'newspaper', 'interviews', 'appearances', 'conferences', 'tours', 'supervising', 'evening', 'camps', 'sponsorships', 'election', 'sophisticated', 'tools', 'entrusted', 'seismic', 'followed', 'covers', '120', 'hydraulic', 'jacks', 'adjustments', 'restore', 'billions', 'instant', 'hindered', 'confided', 'meldrum', 'tube', 'frame', 'road', 'succeeds', 'paper', 'jcmt', 'suit', 'needs', 'delivered', 'arose', 'supervised', 'spectrograms', 'binney', 'repulsive', 'counterbalances', 'exceed', 'braking', 'attempts', 'push', 'infinity', 'halo', 'burning', 'football', 'extending', 'recherche', 'scientifique', 'consultant', 'commissariat', 'ãƒâ\\xa0', 'lã¢â‚¬â„¢ãƒâ©nergie', 'atomique', 'saclay', '1879', 'improvement', 'bulb', 'diminutive', 'minuscule', 'starã¢â‚¬â„¢s', 'cedar', 'extended', 'cover', 'area', '65', 'square', 'sheet', 'polished', 'tin', 'bitumen', 'fruitful', 'alpaca', 'astrophysicsã¢â‚¬â¨ã¢â‚¬â¨', 'ã¢â‚¬â¨ã¢â‚¬â¨jean', 'talks', 'programs', 'unesco', 'dedicates', 'section', 'conservation', 'purity', 'declaration', 'rights', 'generations', 'off', 'rising', 'assemble', 'group', 'spectroscopists', 'specialities', 'portions', 'solid', 'observatorythe', 'receive', 'accepting', 'believed', 'incredibly', 'billion', 'should', 'usable', 'purposes', 'completion', 'starting', '2007', 'damien', 'extraordinary', 'going', 'yerkes', 'chicago', 'extensive', 'repercussions', 'striving', 'towards', 'noãƒâ«l', 'marie', 'paymal', 'lerebours', 'recorded', 'soleil', 'hosts', 'definition', 'movie', 'evenings', 'lands', 'asset', 'library', 'waimea', 'effects', '1618', 'begun', 'reports', 'sent', 'back', 'amazing', 'taking', '182', 'p', 'ejecting', 'devices', 'manually', 'arduous', 'bands', 'verbatim', 'journey', 'twofold', 'objective', 'aeronomy', 'undertook', 'executive', 'learning', 'capabilities', '1987and', 'looked', 'revealed', 'smaller', 'liberate', 'revelation', 'proved', 'nucleosynthesis', 'cartoon', 'franãƒâ§ais', 'represent', 'danger', 'relocated', 'subatomic', 'particle', 'blackman', 'lowell', 'supported', 'metallic', 'wires', 'formed', '1998', 'cuvit', 'uv', 'plastic', 'case', 'examine', 'conditions', 'extragalactic', 'continually', 'decreasing', 'distinguished', 'thirds', 'canadians', 'enemy', 'easily', 'destined', 'norwegian', 'germanic', 'god', 'alone', 'go', 'kodak', 'machine', 'coating', 'plates', 'emulsion', '1849', 'abundant', 'reserve', 'administrative', 'offices', 'spitzer', 'planck', 'selected', 'relative', 'isolation', 'distinction', 'entered', 'topographer', '1872', 'distribute', 'ohio', 'graduated', 'informal', 'discussions', 'hosted', 'concerned', 'phenomena', 'properties', 'brilliance', 'corona', 'uppermost', 'rasmus', 'bartholin', 'splitting', 'icelandic', 'spar', 'goth', 'jesuit', 'priest', 'pierre', 'intention', 'roof', 'egyptians', 'ago', 'coworkers', 'cowley', '54', 'surprise', 'dimmer', 'stronger', 'luminous', 'unmarked', 'areas', 'metal', 'qualities', 'optimal', 'monica', 'daniel', 'nadeau', 'prehistoric', 'seasons', 'length', 'plan', 'hunting', 'ã¢â‚¬â¨during', 'renaissance', 'advances', 'invention', 'observational', 'rise', 'invited', 'speaker', 'annual', 'meeting', 'societies', 'commemorates', 'appreciation', 'subjected', 'produced', 'tends', 'align', 'magnet', 'chatham', 'farm', 'telescopic', 'pursue', 'visit', 'originate', 'belt', 'oort', 'cloud', 'adjoint', 'signal', 'electronically', 'transmitted', 'designing', 'rare', 'event', 'strives', 'interferometry', 'owen', 'sound', 'ã¢â‚¬â¨in', 'defined', 'serious', 'amateurs', 'participate', 'exciting', 'ventilation', 'sixth', 'shares', 'favourable', 'nights', 'winter', 'sometimes', 'jersey', 'give', 'herself', 'equations', 'reputation', 'occurred', 'exploring', 'origins', 'arrive', 'closure', 'moment', 'meridian', '1920ã¢â‚¬â„¢s', 'dissuaded', 'scarcity', 'jobs', 'recent', 'developments', 'visitorã¢â‚¬â„¢s', 'mysterious', 'grandiose', 'story', 'glimmering', 'immensity', 'chlorine', 'thinning', 'setting', 'impetus', 'pressing', 'supply', 'temporal', 'topographic', '360', 'lãƒâ©vis', 'outputs', 'agency', 'agreed', '1890', 'w', 'e', 'g', 'deville', 'o', 'klotz', 'bylaw', 'regulate', 'adopted', 'laboratories', 'abatement', 'initiated', 'hemispheres', \"technology's\", '1961', 'evident', 'efficient', 'yvon', 'georgelin', 'boulesteix', 'fabry', 'perot', 'interferometer', 'deduced', '270', 'antiquity', 'occupation', '1898', 'continuing', 'movements', 'tectonic', 'plate', 'former', 'proportions', 'cobe', 'slight', 'nowadays', 'strong', 'always', 'examined', 'context', 'regina', 'commenced', 'opportunity', 'simultaneously', 'undergraduate', '1902', 'piece', 'textm', 'permanently', 'buildingã¢â‚¬â„¢s', 'protects', 'wind', 'goddard', 'flight', 'center', 'greenbelt', 'maryland', 'spectrograph', 'plumes', 'strongly', 'reflective', 'mercury', 'curves', 'paraboloid', 'concentrate', 'focal', 'achieved', 'held', 'title', 'secured', 'fourth', 'support', 'subterranean', '70th', 'birthday', 'doctorate', 'craters', 'doing', 'meteorite', 'bombardment', 'asteroseismologists', 'hamburg', 'motivated', 'growing', 'officially', '1920', 'lifetime', 'contact', 'clavius', 'engineering', 'labrador', 'interpretation', 'affectionately', 'smallwood', 'establishes', 'quebecã¢â‚¬â„¢s', 'ãƒå½le', 'jãƒâ©sus', 'synthesis', 'hours', '74', 'release', 'quantities', 'dominique', 'franãƒâ§ois', 'arago', 'polariscope', 'edge', '2011', 'hide', 'sectors', 'cores', 'contain', 'flemish', 'painter', 'jan', 'eyck', 'painting', 'entitled', 'crucifixion', '1887', 'pressure', 'subdwarf', 'african', 'leonardo', 'da', 'vinci', 'draws', 'procyon', 'contradicts', 'ideas', 'rethink', 'helping', '94', 'recognize', 'uniformly', 'locally', 'discern', 'source', 'organized', 'cretaceous', 'tertiary', 'extinction', 'disappearance', 'dinosaurs', 'assist', 'stimulate', 'publicã¢â‚¬â„¢s', 'universities', 'continuous', 'forward', 'simple', 'trying', 'directions', 'bodies', 'campaigning', 'get', '2917', 'professors', 'beaudet', 'michaud', 'ejections', 'non', 'irregular', 'manner', 'woodstock', 'hamilton', 'provincial', 'governments', 'approved', 'lord', 'rosse', 'costs', 'planed', 'conscientious', 'acquisition', 'gradually', 'ottawaã¢â‚¬â„¢s', 'records', 'report', 'gift', 'baker', 'nunn', 'armed', '101', 'minutes', 'poles', '1497', 'explored', 'further', 'inland', '1534', 'undoubtedly', 'locate', 'landmass', 'accomplishments', 'identify', 'molecule', 'influence', 'ecology', 'period', 'minimum', 'hyacinthe', 'july', 'telescopeã¢â‚¬â„¢s', 'component', '325', '1875', 'universityã¢â‚¬â„¢s', 'gold', 'succumbing', 'incurable', 'disease', 'acted', 'priddis', 'locality', 'southwest', 'lot', 'existing', 'interpret', 'things', 'radicals', 'intermediary', 'exist', 'emits', 'album', 'groups', 'calibrate', '1953', '1903', 'therein', 'difficulty', 'berg', 'wassenaar', 'showing', 'observers', 'outdoors', 'techniques', 'polychrome', 'version', 'independently', 'invented', 'godfrey', 'brett', 'gladman', 'kavelaars', 'prospero', 'setebos', 'stephano', 'eight', 'saturn', 'siarnaq', 'tarvos', 'ijrak', 'thrymr', 'skathi', 'mundilfari', 'erriapo', 'suttungr', 'user', 'fees', 'paid', 'agencies', 'twenty', 'earlier', 'penzias', 'forefront', 'preparation', 'wealthy', 'rancher', 'donated', 'wilhelm', 'bunsen', 'gustav', 'kirchhoff', 'co', 'authors', 'neptune', 'announced', 'greatest', 'blocks', 'lives', 'drives', 'concepts', 'teachers', 'overview', 'hhcc', 'web', 'virtual', 'page', 'canso', 'caltech', 'subsequent', '1938', 'institution', 'convert', 'surplus', 'nuclei', 'relatively', 'break', 'travel', 'too', 'span', 'financial', 'cease', 'operations', 'standard', 'collaborated', 'explain', 'helium', 'depression', 'hit', 'find', 'wood', 'marquis', 'cogolin', 'dates', '1751', 'pieces', 'increasing', 'popularity', 'lectures', 'appeared', 'films', 'speaking', 'paramount', 'importance', 'hypotheses', 'either', 'insufficient', 'voyage', 'phenomenon', 'mean', 'reaching', 'originally', 'harvard', 'hand', 'hagermanã¢â‚¬â„¢s', 'corners', 'finland', 'daunting', 'assembling', 'scottish', 'groundbreaking', 'magnetism', 'classic', 'shaped', 'minimize', 'effect', '092', 'exists', 'dans', 'lã¢â‚¬â„¢azur', 'steps', 'immediate', 'acclaim', '220', 'medium', 'content', 'sherbrooke', 'michel', 'fich', '1884', 'parque', 'el', 'leoncito', '13', 'mounted', 'platform', 'pointed', 'vibrate', 'lifespan', 'reliable', 'envelopes', 'direction', 'remarkable', 'proving', 'bruce', 'campbell', 'ã¢â‚¬â¨ã¢â‚¬â¨renãƒâ©', 'sustains', 'exposure', 'guided', 'authorities', 'leave', 'refuge', 'balloon', 'carried', 'altitude', 'avoid', 'initially', 'delays', 'postponements', 'specializing', 'assignment', 'bristol', 'improvements', 'discovering', 'confirming', 'decade', 'excavated', 'earliest', 'ancestors', 'spread', 'characterized', 'person', 'passionate', 'inside', 'must', 'namely', 'framework', 'montrãƒâ©al', 'tololo', '368', 'esquimalt', 'nominated', 'commissioner', 'boundary', 'border', 'utrecht', 'harriot', 'galileo', 'performs', 'feat', 'scholarship', 'carnegie', 'pasadena', 'rochester', 'york', 'colony', 'rigorous', 'standards', 'domains', 'collections', 'spherical', 'containing', 'millions', 'inhabitants', 'contemporary', 'vison', 'terence', 'dickinson', 'splendors', '40', 'synchronizes', 'erupted', 'navy', 'parliament', 'freed', 'funds', 'aftermath', 'copernicus', 'dense', 'escape', 'pull', 'attractive', 'hundred', 'climate', 'agriculture', 'fishing', 'howard', 'levy', 'ball', 'signalled', 'midday', 'dropped', 'nearing', 'bigger', 'ended', 'reclaimed', 'ambitious', 'trips', 'headed', 'contingent', '300', 'halleyã¢â‚¬â„¢s', 'comet', 'peruvian', 'consisting', 'series', 'algernon', 'pearce', 'unresolved', 'responded', 'erecting', 'ã¢â‚¬â¨ã¢â‚¬â¨john', 'rival', 'frequency', 'per', 'hour', 'brightness', 'doubled', 'attained', 'rank', 'lieutenant', 'commander', 'michael', '64', 'derived', 'greek', 'root', 'astron', 'nomos', 'arrangement', 'law', 'planar', 'geometry', '1667', 'louis', 'xiv', 'commands', 'roads', 'capital', 'lit', 'combat', 'thefts', 'crimes', 'refine', 'scanning', 'noon', 'founding', 'attracted', 'improved', 'institutionã¢â‚¬â„¢s', 'seismological', 'modernized', 'heat', 'suddenly', 'brighten', '3314', 'honours', 'rand', 'africa', 'variety', 'methods', 'assisted', 'geographic', 'maps', 'affecting', '1957', 'spiral', 'enrichment', 'winds', 'explores', '2004', 'regularly', 'media', 'willis', 'ritchey', '80', 'warren', 'rue', 'tackled', 'doppler', 'thermonuclear', '4843', 'convinced', 'incorrect', 'conviction', 'foray', '39', 'camp', 'forrest', 'military', 'tennessee', 'extremely', 'helps', 'microvariability', 'oscillation', 'forms', 'depletes', 'fuel', 'helenã¢â‚¬â„¢s', 'husband', 'occupy', '1936', 'osiris', 'gases', 'aerosols', 'isaac', 'calibrated', 'emanating', 'itã¢â‚¬â„¢s', 'illinois', 'michigan', 'unaided', 'eye', 'wonderful', 'success', 'translated', 'languages', 'dynamic', 'accounts', 'mãƒâ©ganticã¢â‚¬â„¢s', 'productivity', 'missionaries', '1604', '3316', 'depends', 'suggest', 'twins', 'latin', 'ian', 'shelton', 'chilean', 'magellan', 'consequently', 'offer', 'electrically', 'neutral', 'electrical', 'east', 'leonard', 'digges', 'constructs', 'attention', 'detail', 'analogous', 'seismology', 'geophysicists', 'earthquakes', 'reach', 'tex', 'does', 'farmers', 'plant', 'harvest', 'navigators', 'sail', 'oceans', 'ã¢â‚¬â¨ã¢â‚¬â¨hubert', 'tiny', 'messier', 'needed', 'minimal', 'starts', '1765', 'weights', 'metric', 'ton', 'teamed', 'garrison', 'propose', 'timeline', 'hde', '226868', 'wobble', 'if', 'orbiting', 'decision', '00035', 'analyzes', 'interprets', 'gathered', 'integrates', 'simulations', 'ranger', 'vii', 'mostly', 'editor', 'newsletter', 'keep', 'notre', 'dame', 'des', 'bois', 'primary', 'analyse', 'tries', 'accelerated', 'nature', 'introduced', 'increase', 'acadia', 'surprising', 'synchronize', 'silicon', 'carbide', 'fluoride', 'aluminum', 'enthusiastic', 'pressuring', 'reality', 'scarborough', 'erindale', 'thesis', 'subject', '1934', 'backyard', 'applied', 'trace', 'led', 'expedition', 'australia', 'verify', 'einsteinã¢â‚¬â„¢s', 'relativity', 'concerning', 'deflection', 'accomplished', 'recording', 'roots', 'hold', 'samuel', 'governor', 'goldreich', 'kept', 'acrylic', '17', 'geodesic', '9', 'detectors', 'sense', '1895', '33', 'fulfill', '162', 'family', 'immigrated', '1862', 'settled', 'hope', 'device', '1906', 'lack', 'prompted', 'campaign', 'winnipeg', 'manitoba', 'oschin', 'lemayã¢â‚¬â„¢s', 'passions', 'himself'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tokenizer.word_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "42689f86-6fdd-4a34-b9ec-4b532e33d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence1 = 'astronomers study stars'\n",
    "# sentence2 = 'astronomical space research'\n",
    "sentence1 = 'astronomers study stars'\n",
    "sentence2 = 'columbia large department'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "16dc484e-9bdb-40fd-975b-defbe5928997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[72, 71, 73]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tokenizer.texts_to_sequences([sentence2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "06a83595-a686-4201-a3d8-fcd16ad3985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = tf.constant(enc_tokenizer.texts_to_sequences([sentence1]))\n",
    "inputs2 = tf.constant(enc_tokenizer.texts_to_sequences([sentence2]))\n",
    "output1 = transformer.encoder(inputs1)[0]\n",
    "output2 = transformer.encoder(inputs2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4097e028-e79b-4468-a51f-3230ce1002db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 20), dtype=float32, numpy=\n",
       "array([[[-0.67756903,  1.346677  , -0.5204264 , -0.46391895,\n",
       "         -0.10949869, -1.0996535 , -1.6299821 , -0.89470893,\n",
       "          0.85599315, -2.0965676 ,  0.3229088 ,  0.62217706,\n",
       "          0.00554255,  1.4535601 ,  1.2204055 ,  0.04139756,\n",
       "          0.5306589 ,  0.62773937,  0.5623679 , -0.1215599 ],\n",
       "        [-0.6787881 ,  1.3456157 , -0.5213721 , -0.46405748,\n",
       "         -0.10739103, -1.0955036 , -1.6484089 , -0.88720775,\n",
       "          0.8426463 , -2.088242  ,  0.30834907,  0.62335247,\n",
       "          0.01012551,  1.4568517 ,  1.2244682 ,  0.0489862 ,\n",
       "          0.5317003 ,  0.6378933 ,  0.55352044, -0.11633141],\n",
       "        [-0.6672824 ,  1.3579967 , -0.513648  , -0.4805963 ,\n",
       "         -0.11982363, -1.0853963 , -1.6426896 , -0.86946636,\n",
       "          0.86727303, -2.0912907 ,  0.2852972 ,  0.61864984,\n",
       "         -0.00210325,  1.4409746 ,  1.2625933 ,  0.04186494,\n",
       "          0.5309469 ,  0.63348925,  0.5315108 , -0.12131207]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0a6f8c6f-2bad-408b-8fcb-d8d933edb85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 20), dtype=float32, numpy=\n",
       "array([[[-0.7490195 ,  1.427113  , -0.68845206, -0.3570645 ,\n",
       "         -0.13416201, -1.0223509 , -1.5686895 , -0.71513605,\n",
       "          0.69024277, -2.2789195 ,  0.37676978,  0.644699  ,\n",
       "          0.14873789,  1.3522933 ,  1.2267828 ,  0.03221232,\n",
       "          0.40405414,  0.61595446,  0.61161333, -0.04473148],\n",
       "        [-0.7456242 ,  1.4265598 , -0.68790305, -0.3658358 ,\n",
       "         -0.13345943, -1.0127387 , -1.566251  , -0.70938325,\n",
       "          0.6936764 , -2.2849908 ,  0.36748698,  0.6420739 ,\n",
       "          0.14543633,  1.3456935 ,  1.2424892 ,  0.03324926,\n",
       "          0.40752795,  0.618866  ,  0.60586077, -0.05030824],\n",
       "        [-0.74507993,  1.4317414 , -0.68603605, -0.36621174,\n",
       "         -0.13867924, -1.0073918 , -1.5649184 , -0.7063888 ,\n",
       "          0.6960495 , -2.286459  ,  0.35911527,  0.63833314,\n",
       "          0.14775106,  1.3480332 ,  1.2405849 ,  0.02890521,\n",
       "          0.41022167,  0.6205619 ,  0.6054014 , -0.05310567]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a183a9a1-0f1f-48b6-b556-cf77a681da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "18e498dd-837a-48c1-aae7-b82bb966f47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9938858350118002"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cos_sim(output1[0,0], output2[0,0]) + cos_sim(output1[0,1], output2[0,1]) + cos_sim(output1[0,2], output2[0,2])) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39189e58-b029-4e87-add7-8f5f1a2b1a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9991a7e-57ed-4e4e-a282-8a0f0a0547d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
