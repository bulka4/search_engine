{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de30dcbd-b243-4bcb-809f-f0c971667e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18893da0-a7f7-4f80-a489-9aff6d0dee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68f9ca4-bb01-4ad8-8532-c999a4317396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98de543b-50f6-4786-b443-9071c3c21a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searched_sentence = 'information about empower users'\n",
    "# searched_sentence = 'information about empower employees'\n",
    "# searched_sentence = 'translators worked hours'\n",
    "# searched_sentence = 'how much translators worked'\n",
    "# searched_sentence = 'data about translators work efficiency'\n",
    "# searched_sentence = 'cant see productivity data'\n",
    "# searched_sentence = 'productive hours worked not visible'\n",
    "# searched_sentence = 'data about vendors'\n",
    "searched_sentence = 'data about suppliers'\n",
    "\n",
    "# sentences = [\n",
    "#     'data about helix purchases', 'data about empower transactions', 'data about helix jobs',\n",
    "#     'data about employees names', 'information about workers surnames', 'data about employees names',\n",
    "#     'data about empower purchases', 'data about helix procurements'\n",
    "# ]\n",
    "sentences = pd.read_excel(os.path.join(os.path.dirname(os.getcwd()), 'data', 'documentation.xlsx')).values.flatten()\n",
    "sen_similarities = pd.DataFrame()\n",
    "\n",
    "for sentence in sentences:\n",
    "    similarity = cos_sim(model([sentence])[0], model([searched_sentence])[0])\n",
    "    new_row = pd.DataFrame([[searched_sentence, sentence, similarity]])\n",
    "    sen_similarities = pd.concat((sen_similarities, new_row))\n",
    "        \n",
    "sen_similarities.reset_index(inplace = True, drop = True)\n",
    "sen_similarities.columns = ['searched_sentence', 'sentence', 'similarity_score']\n",
    "sen_similarities = sen_similarities.sort_values(by = 'similarity_score', ascending = False)\n",
    "sen_similarities.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28bd780c-8386-4f07-a217-c08341766f81",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0)  in order to allow someone to see someone's productivity data we need to adjust data security rules in elasticube \n",
      "\n",
      "1)  data about how much productive hours or words someone has is taken from the table stage.emp.tbl_TSProductivityTimeSpans and stage.emp.tbl_TSProductivityEntries \n",
      "\n",
      "2)  table Stage.emp.tblLU_CostControlCodes contains information about cost codes \n",
      "\n",
      "3)  table Stage.emp.tblLU_CostCenters contains information about cost centers \n",
      "\n",
      "4)  in order to prepare data for headcount dashboard we need to use ssis package Corpsql12>DBI Manual Updates/HEADCOUNT_NEW which creates table Stage.dbi.Headcount_NEW which is used in elasticube \n",
      "\n",
      "5)  table stage.dbi.Headcount_NEW contains information about employees who are working for us, who joined us and who leaved us \n",
      "\n",
      "6)  table stage.helix.JT_External_PO_v2 contains information about purchase orders from Helix \n",
      "\n",
      "7)  in order to prepare data for lo dashboard we need to run sql job LXD_Lo_Dashboard on dnaprod \n",
      "\n",
      "8)  tables with data about empower on dnaprod sql server are created by sql jobs emp_to_stage_part1 and part2 \n",
      "\n",
      "9)  table Stage.emp.tblLU_JobTitles contains information about job titles \n",
      "\n",
      "10)  table Stage.dbi.PRTL_Vendor contains information from helix about vendor id, code, name, user name, email, status \n",
      "\n",
      "11)  data about users for headcoun dashboard comes from hr global and empower \n",
      "\n",
      "12)  website with documentation about our company: confluence.sdl.com \n",
      "\n",
      "13)  in order to prepare data for lo dashboard we need to update following tables: Analiza.dbi.NEW_ECPW_EH_2020, Stage.dbi.Headcount_NEW,  Stage.[CustProfit].vw_External_PO \n",
      "\n",
      "14)  Nikhil Marathe might help with data from empower \n",
      "\n",
      "15)  table Stage.emp.tbl_Clients contains information from empower about clients,their names \n",
      "\n",
      "16)  table Stage.emp.tbl_TSProductivityEntries contains information from timesheet about users productivity, productivity measure, number of translated words \n",
      "\n",
      "17)  table dw.fact.Productivity_TIME contains information about how much time employees were working \n",
      "\n",
      "18)  table stage.helix.JT_Job_Target_Langs_v2 contains information about Helix jobs \n",
      "\n",
      "19)  table dw.fact.Productivity_WORDS contains information about how many words employees have translated \n",
      "\n",
      "20)  courses to learn about sisense: sisense.litmos.com \n",
      "\n",
      "21)  table Stage.emp.tblLU_Offices contains information from empower about offices, their managers, region, country, currency, adress, city \n",
      "\n",
      "22)  table stage.helix.JT_Task_v2 contains information about tasks from Helix \n",
      "\n",
      "23)  table Stage.dbi.LangMap contains information from empower and helix about languages, their names, id, code, group \n",
      "\n",
      "24)  table Stage.emp.tblLU_Departments contains information from empower about departments, their managers \n",
      "\n",
      "25)  table Stage.emp.tbl_TSTimesheet contains information from timesheet about activities and operations user perform, their date, cost code \n",
      "\n",
      "26)  table Stage.emp.tblLU_TSProductivityMeasures contains information form timesheet about productivity type, operation, productivity measure \n",
      "\n",
      "27)  table dw.fact.Quality_Evaluations contains information about quality evaluations of performed translations. It is created by ssis package 'Quality Evaluation (Translation)' \n",
      "\n",
      "28)  table Stage.dbi.AdjustmentFactor_Map contains information about adjustment factors for calculating adjusted words for different tasks and units \n",
      "\n",
      "29)  table Stage.emp.tbl_TSProductivityTimeSpans contains information from timesheet about productive activities and operations user perform, their date, cost code \n",
      "\n",
      "30)  table Stage.emp.tbl_Users contains information from empower about employees and their name, email, cost center, office, department, job title, manager, join date \n",
      "\n",
      "31)  table Stage.emp.tblLU_TSOperationTypes contains information about timesheet's operations and activities types \n",
      "\n",
      "32)  Joe longworth is dealing with Helix data \n",
      "\n",
      "33)  we give people access to dashboards by assigning them to groups. There are scripts running every week adding and removing people from some of those groups. \n",
      "\n",
      "34)  table Stage.emp.tblLU_TSOperations contains information about timesheet's operations, activities, their type \n",
      "\n",
      "35)  Empowerâ€™s website: empower.sdl.com  \n",
      "\n",
      "36)  Helix documentation: https://sdl.appiancloud.com/suite/sites/data-dictionary \n",
      "\n",
      "37)  Website for raising Sisense tickets to ask for help with problems and to get answers for questions: community.sisense.com \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sen_similarities.loc[:, 'sentence']):\n",
    "    print(f'{i}) ', sentence, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65523546-3a6e-46fc-82b7-035797838fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6253435"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(model(['stair'])[0], model(['escalator'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b298c2f-912b-43d0-8998-3407addf333f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a086c1-1df3-4861-9903-c172db36349d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ce8d2-a0cc-46f6-af9a-09466a058752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fbaa65-099e-4cc4-9a9c-43899655b99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7bf169cf-aa1a-4699-92c5-91c1186fa161",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 6.1741 Accuracy 0.0000\n",
      "Epoch 2 Batch 0 Loss 6.1098 Accuracy 1.0000\n",
      "Epoch 3 Batch 0 Loss 6.0834 Accuracy 1.0000\n",
      "Epoch 4 Batch 0 Loss 6.0416 Accuracy 1.0000\n",
      "Epoch 5 Batch 0 Loss 5.9871 Accuracy 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14296/3912870223.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# Iterate over the training dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_number\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# Print training progress every few batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14296/3912870223.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(inputs, targets, model, optimizer, loss_function, train_loss, train_accuracy)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Apply the gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Update the loss and accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    671\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m         return self._distributed_apply(strategy, grads_and_vars, name,\n\u001b[1;32m--> 673\u001b[1;33m                                        apply_state)\n\u001b[0m\u001b[0;32m    674\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m         return tf.distribute.get_replica_context().merge_call(\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[1;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[0;32m    720\u001b[0m               var.op.name):\n\u001b[0;32m    721\u001b[0m             update_op = distribution.extended.update(\n\u001b[1;32m--> 722\u001b[1;33m                 var, apply_grad_to_update_var, args=(grad,), group=False)\n\u001b[0m\u001b[0;32m    723\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_cross_replica_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m               \u001b[1;31m# In cross-replica context, extended.update returns a list of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2632\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m   2633\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2634\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2635\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2636\u001b[0m       return self._replica_ctx_update(\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3707\u001b[0m     \u001b[1;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3708\u001b[0m     \u001b[1;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3709\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3711\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   3713\u001b[0m     \u001b[1;31m# once that value is used for something.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3714\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3715\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3716\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3717\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    599\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m    698\u001b[0m           \u001b[0mapply_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"apply_state\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m         return self._resource_apply_sparse_duplicate_indices(\n\u001b[1;32m--> 700\u001b[1;33m             grad.values, var, grad.indices, **apply_kwargs)\n\u001b[0m\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m\"apply_state\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_apply_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_resource_apply_sparse_duplicate_indices\u001b[1;34m(self, grad, handle, indices, **kwargs)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         values=grad, indices=indices)\n\u001b[0;32m   1285\u001b[0m     return self._resource_apply_sparse(summed_grad, handle, unique_indices,\n\u001b[1;32m-> 1286\u001b[1;33m                                        **kwargs)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_resource_apply_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\keras\\optimizer_v2\\adam.py\u001b[0m in \u001b[0;36m_resource_apply_sparse\u001b[1;34m(self, grad, var, indices, apply_state)\u001b[0m\n\u001b[0;32m    215\u001b[0m       \u001b[0mv_sqrt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m       var_update = tf.compat.v1.assign_sub(\n\u001b[1;32m--> 217\u001b[1;33m           \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoefficients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mm_t\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mv_sqrt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcoefficients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epsilon'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m           use_locking=self._use_locking)\n\u001b[0;32m    219\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar_update\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_t\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_step(inputs, targets, model, optimizer, loss_function, train_loss, train_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Pass the inputs through the transformer\n",
    "        # last word in targets is <end> token\n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        # first word in targets is the <start> token\n",
    "        loss = loss_function(targets, predictions)\n",
    "    \n",
    "    # Apply the gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  \n",
    "    # Update the loss and accuracy\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets, predictions)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Initialize the metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 1\n",
    "\n",
    "inputs = ['data about helix purchases', 'data about empower transactions']\n",
    "targets = tf.random.uniform((2, 1), maxval = 512, dtype = tf.int32)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    # Iterate over the training dataset\n",
    "    for batch_number in range(len(inputs) // batch_size + 1):\n",
    "        train_step(inputs, targets, model, optimizer, loss_function, train_loss, train_accuracy)\n",
    "      \n",
    "        # Print training progress every few batches\n",
    "        if batch_number % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch_number} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d089b085-3639-4dc5-af9d-21eecb714870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
