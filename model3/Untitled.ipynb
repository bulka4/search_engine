{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3fb4c2-6cc7-48f1-95da-16865ee4db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5c1c7f-32ca-4db4-a20f-c24764eb1fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(string: str, \n",
    "               punctuations = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "               stop_words = stopwords.words('english'),\n",
    "               # porter = PorterStemmer()\n",
    "               wnl = WordNetLemmatizer()\n",
    "              ):\n",
    "    \"\"\"\n",
    "    A method to clean text. It removes punctuations, stop words, applies lemmatization.\n",
    "    \"\"\"\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # stemming/lemmatizing words. That means changing word to its basic format, for example\n",
    "    # words 'fishing', 'fished', 'fischer' will be changed into a word 'fisch'\n",
    "    # lemmatization should be better because stemming changes words too much, for example\n",
    "    # business is changed into busi\n",
    "    # string = ' '.join([porter.stem(word) for word in string.split()])\n",
    "    string = ' '.join([wnl.lemmatize(word, pos = \"v\") for word in string.split()])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string\n",
    "\n",
    "def create_training_data(tokenizer,\n",
    "                         sentences_file,\n",
    "                         # embed_matrix_file,\n",
    "                         model_folder,\n",
    "                         max_sen_len = None\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Creating a training and testing datasets self.x_train, self.x_test, self.y_train, self.y_test. This function\n",
    "    also creates and saves a tokenizer.\n",
    "    \"\"\"\n",
    "    sentences_tables = pd.read_excel(sentences_file).values\n",
    "    random.shuffle(sentences_tables)\n",
    "    clean_sentences = np.array([clean_text(sentence) for sentence in sentences_tables[:, 0]])\n",
    "\n",
    "    tokenizer.fit_on_texts(clean_sentences)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(clean_sentences)\n",
    "    if max_sen_len == None:\n",
    "        max_sen_len = np.max([len(seq) for seq in sequences])\n",
    "    x = pad_sequences(sequences, maxlen = max_sen_len)\n",
    "\n",
    "    # embed_matrix = pd.read_csv(embed_matrix_file).values\n",
    "\n",
    "    x_train, x_test = train_test_split(x, test_size = 0.2)\n",
    "\n",
    "    with open(os.path.join(model_folder, 'tokenizer.json'), 'w') as file:\n",
    "        json.dump(tokenizer.to_json(), file)\n",
    "        \n",
    "    return x_train, x_test\n",
    "\n",
    "\n",
    "def get_coefs(word, *arr): \n",
    "    return word, list(np.asarray(arr, dtype='float'))\n",
    "\n",
    "\n",
    "def create_embedding_file(tokenizer,\n",
    "                          embed_file_src = r'model\\glove.840B.300d.txt', \n",
    "                          embed_file_trg = r'model\\model_embeddings.txt'\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    This function will create an embedding file called embed_file_trg which will contain only those words \n",
    "    from embed_file_src which are present in the training dataset (tokenizer.word_index).\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = dict(get_coefs(*o.split(\" \")) for o in open(embed_file_src, errors = 'ignore'))\n",
    "    with open(embed_file_trg, 'w') as file:\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            word_vector = embeddings[word]\n",
    "            line = ' '.join(np.concatenate([[word], word_vector]))\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "\n",
    "def create_embedding_matrix(tokenizer,\n",
    "                            model_folder,\n",
    "                            word_vec_dim,\n",
    "                            embed_file_path,\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    A function to create an embedding matrix. This is a matrix where each row is a vector representing a word.\n",
    "    To create that matrix we use a word embedding file which path is equal to embedding_file_path.\n",
    "    embedding_matrix[row_number] is a vector representation for a word = list(tokenizer.word_index.keys())[row_number - 1]\n",
    "    First row of embedding_matrix are zeros. This matrix is needed to train a model.\n",
    "    \"\"\"\n",
    "    embeddings = dict(get_coefs(*o.split(\" \")) for o in open(embed_file_path, errors = 'ignore'))\n",
    "\n",
    "    # embedding_matrix[row_number] is a vector representation of a word = self.tokenizer.word_index.keys()[row_number - 1]\n",
    "    # first row in embedding_matrix is 0\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_counts) + 1, word_vec_dim))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index > len(tokenizer.word_counts):\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                embedding_matrix[index] = embeddings[word]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    pd.DataFrame(embedding_matrix).to_csv(os.path.join(model_folder, 'embedding_matrix.csv'))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a6c030-6b64-4d53-9cdf-18ae47d5b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# max_sen_len = 20\n",
    "sentences_file = r'data\\sentences_tables.xlsx'\n",
    "embed_matrix_file = r'model\\embedding_matrix.csv'\n",
    "model_folder = 'model'\n",
    "word_vec_dim = 300\n",
    "embed_file_path = r'model\\model_embeddings.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa21b414-1a9d-4045-b551-9416b28cf16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = create_training_data(\n",
    "    tokenizer = tokenizer, \n",
    "    # max_sen_len = max_sen_len,\n",
    "    sentences_file = sentences_file,\n",
    "    # embed_matrix_file = embed_matrix_file,\n",
    "    model_folder = model_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703325ad-5693-45fb-8ca5-0632a439fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbd939a-fe1a-4b2d-9b3b-b46858c3282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = create_embedding_matrix(\n",
    "    tokenizer = tokenizer,\n",
    "    model_folder = model_folder,\n",
    "    word_vec_dim = word_vec_dim,\n",
    "    embed_file_path = embed_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a69efdd-4542-4dfc-8cb3-d65237cdf61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [-0.50318 ,  0.27905 , -0.045497, ...,  0.4781  ,  0.13005 ,\n",
       "        -0.014399],\n",
       "       [-0.89423 ,  0.39636 ,  0.64359 , ..., -0.15076 ,  0.06987 ,\n",
       "         0.041258],\n",
       "       ...,\n",
       "       [ 0.37492 , -0.052425, -0.60094 , ..., -0.36104 , -0.065253,\n",
       "        -0.1206  ],\n",
       "       [ 0.012832,  0.22669 , -0.17511 , ...,  0.17134 ,  0.040047,\n",
       "        -0.37131 ],\n",
       "       [-0.39054 , -0.55117 , -0.073466, ...,  0.34569 ,  0.30918 ,\n",
       "        -0.32873 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8b29fc-9891-4abb-8213-07ab482b117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(embed_matrix):\n",
    "    if (row == np.zeros(300)).all():\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8854e78-9e2f-4fde-973c-209d545335c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('employee', 1)\n",
      "('cost', 2)\n",
      "('user', 3)\n",
      "('office', 4)\n",
      "('business', 5)\n",
      "('unit', 6)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(tokenizer.word_index.items()):\n",
    "    print(item)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "130632c9-bbf4-4a09-a7e1-e8c153935bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self,\n",
    "                 embedding_dim,\n",
    "                 lstm_out_size,\n",
    "                 batch_size,\n",
    "                 embed_matrix\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.lstm_out_size = lstm_out_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = layers.Embedding(\n",
    "            input_dim = embed_matrix.shape[0],\n",
    "            output_dim = embedding_dim,\n",
    "            embeddings_initializer = tf.keras.initializers.Constant(embed_matrix),\n",
    "            trainable = False\n",
    "        )\n",
    "        self.lstm = layers.LSTM(\n",
    "            units = self.lstm_out_size,\n",
    "            return_sequences = True,\n",
    "            return_state = True\n",
    "        )\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x, state_h = None, state_c = None):\n",
    "        # x.shape = (batch_size, max_sen_len)\n",
    "        # x is a series of numbers which represent words\n",
    "        # state_h.shape = (batch_size, lstm_out_size)\n",
    "        \n",
    "        if state_h == None or state_c == None:\n",
    "            state_h, state_c = self.initialize_hidden_state()\n",
    "        \n",
    "        # make sure that the types are correct\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        state_h = tf.cast(state_h, tf.float32)\n",
    "        state_c = tf.cast(state_c, tf.float32)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        # x.shape after embedding = (batch_size, max_sen_len, embedding_dim)\n",
    "        # output.shape = (batch_size, max_sen_len, lstm_out_size)\n",
    "        # state_h.shape = (batch_size, lstm_out_size)\n",
    "        output, state_h, state_c = self.lstm(x, initial_state = [state_h, state_c])\n",
    "        return output, state_h, state_c\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        state_h = tf.zeros((self.batch_size, self.lstm_out_size))\n",
    "        state_c = tf.zeros((self.batch_size, self.lstm_out_size))\n",
    "        return state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e07b1daa-cfc1-4cfe-85a6-a58ce5120f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(embedding_dim = 300,\n",
    "                 lstm_out_size = 10,\n",
    "                 batch_size = 2,\n",
    "                 embed_matrix = embed_matrix\n",
    "                 )\n",
    "\n",
    "x = np.array([[1, 2], [1, 2]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    output, state_h, state_c = encoder(x)\n",
    "    \n",
    "variables = encoder.trainable_variables\n",
    "gradients = tape.gradient(output, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "936a4350-b1cc-44bb-8e82-74be79bd3b83",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(300, 40), dtype=float32, numpy=\n",
       " array([[ 3.91362980e-02,  1.78501666e-01, -6.65536746e-02, ...,\n",
       "          2.27576867e-03,  3.06596979e-03, -5.52950427e-03],\n",
       "        [-2.44583692e-02, -1.00564852e-01,  4.86165360e-02, ...,\n",
       "          4.39209789e-02,  3.47434767e-02,  3.46905664e-02],\n",
       "        [ 7.62790767e-03,  1.84747055e-02, -2.33989414e-02, ...,\n",
       "         -6.68730587e-02, -5.38272634e-02, -4.74490821e-02],\n",
       "        ...,\n",
       "        [-4.10167016e-02, -1.71791986e-01,  7.95203224e-02, ...,\n",
       "          6.06815591e-02,  4.77755107e-02,  4.92389351e-02],\n",
       "        [-6.89753098e-03, -4.42978255e-02,  3.52535956e-03, ...,\n",
       "         -5.33669665e-02, -4.33627591e-02, -3.55112329e-02],\n",
       "        [-7.87226111e-03, -2.61273235e-05,  3.63165438e-02, ...,\n",
       "          1.47570401e-01,  1.19062632e-01,  1.03082106e-01]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(10, 40), dtype=float32, numpy=\n",
       " array([[ 6.16146077e-04,  3.51791561e-04, -2.61891261e-03,\n",
       "         -8.49191158e-04, -2.83790007e-03, -7.85931305e-04,\n",
       "          2.48138164e-03, -7.55050592e-03, -5.71352849e-03,\n",
       "         -7.98935350e-03,  5.70962846e-04,  3.45674856e-03,\n",
       "         -1.05071394e-03, -7.94795749e-04, -9.95087554e-04,\n",
       "         -1.64881392e-04,  2.02416768e-03, -1.22586254e-03,\n",
       "         -1.25461491e-03, -1.28804415e-03, -2.40219980e-02,\n",
       "         -2.27704532e-02, -1.61680132e-02, -2.66883299e-02,\n",
       "         -1.08878417e-02, -1.61510017e-02, -2.79187560e-02,\n",
       "         -1.04743456e-02, -9.64249391e-03, -1.07615450e-02,\n",
       "          2.45235395e-03,  5.10128867e-03, -3.97462305e-03,\n",
       "         -2.96703260e-03, -6.38126861e-03, -1.25083548e-03,\n",
       "          6.03190670e-03, -1.01070870e-02, -8.15218035e-03,\n",
       "         -7.07404548e-03],\n",
       "        [ 2.82713096e-03,  1.61416398e-03, -1.20166447e-02,\n",
       "         -3.89643735e-03, -1.30214496e-02, -3.60617531e-03,\n",
       "          1.13855973e-02, -3.46448161e-02, -2.62160115e-02,\n",
       "         -3.66584323e-02,  2.61981180e-03,  1.58609804e-02,\n",
       "         -4.82110679e-03, -3.64684896e-03, -4.56586992e-03,\n",
       "         -7.56543421e-04,  9.28771123e-03, -5.62476041e-03,\n",
       "         -5.75668784e-03, -5.91007480e-03, -1.10222779e-01,\n",
       "         -1.04480177e-01, -7.41854757e-02, -1.22457005e-01,\n",
       "         -4.99578826e-02, -7.41074160e-02, -1.28102705e-01,\n",
       "         -4.80605960e-02, -4.42437194e-02, -4.93783839e-02,\n",
       "          1.12524061e-02,  2.34068055e-02, -1.82372015e-02,\n",
       "         -1.36139616e-02, -2.92798784e-02, -5.73934615e-03,\n",
       "          2.76768636e-02, -4.63754609e-02, -3.74055468e-02,\n",
       "         -3.24586220e-02],\n",
       "        [-1.04392448e-03, -5.96033700e-04,  4.43717325e-03,\n",
       "          1.43876823e-03,  4.80819959e-03,  1.33158837e-03,\n",
       "         -4.20415727e-03,  1.27926767e-02,  9.68032144e-03,\n",
       "          1.35362092e-02, -9.67371394e-04, -5.85670257e-03,\n",
       "          1.78020447e-03,  1.34660723e-03,  1.68595777e-03,\n",
       "          2.79355387e-04, -3.42950854e-03,  2.07695551e-03,\n",
       "          2.12566997e-03,  2.18230835e-03,  4.07000072e-02,\n",
       "          3.85795422e-02,  2.73931548e-02,  4.52175252e-02,\n",
       "          1.84470620e-02,  2.73643304e-02,  4.73022126e-02,\n",
       "          1.77464839e-02,  1.63370930e-02,  1.82330795e-02,\n",
       "         -4.15497646e-03, -8.64301529e-03,  6.73412764e-03,\n",
       "          5.02698636e-03,  1.08116614e-02,  2.11926643e-03,\n",
       "         -1.02197444e-02,  1.71242431e-02,  1.38120819e-02,\n",
       "          1.19854202e-02],\n",
       "        [-7.02842022e-04, -4.01291036e-04,  2.98741134e-03,\n",
       "          9.68678098e-04,  3.23721184e-03,  8.96517187e-04,\n",
       "         -2.83052912e-03,  8.61291308e-03,  6.51746104e-03,\n",
       "          9.11351014e-03, -6.51301176e-04, -3.94313643e-03,\n",
       "          1.19855662e-03,  9.06628964e-04,  1.13510317e-03,\n",
       "          1.88081322e-04, -2.30898196e-03,  1.39834976e-03,\n",
       "          1.43114780e-03,  1.46928069e-03,  2.74020564e-02,\n",
       "          2.59744097e-02,  1.84429642e-02,  3.04435603e-02,\n",
       "          1.24198347e-02,  1.84235573e-02,  3.18471156e-02,\n",
       "          1.19481580e-02,  1.09992586e-02,  1.22757675e-02,\n",
       "         -2.79741688e-03, -5.81907481e-03,  4.53387946e-03,\n",
       "          3.38451425e-03,  7.27915671e-03,  1.42683659e-03,\n",
       "         -6.88063726e-03,  1.15292231e-02,  9.29924753e-03,\n",
       "          8.06941185e-03],\n",
       "        [-5.27013850e-04, -3.00901098e-04,  2.24005827e-03,\n",
       "          7.26346392e-04,  2.42736680e-03,  6.72237773e-04,\n",
       "         -2.12242268e-03,  6.45824289e-03,  4.88700438e-03,\n",
       "          6.83360687e-03, -4.88366815e-04, -2.95669213e-03,\n",
       "          8.98716738e-04,  6.79819903e-04,  8.51137273e-04,\n",
       "          1.41029494e-04, -1.73134974e-03,  1.04852812e-03,\n",
       "          1.07312121e-03,  1.10171444e-03,  2.05469541e-02,\n",
       "          1.94764584e-02,  1.38291344e-02,  2.28275731e-02,\n",
       "          9.31279641e-03,  1.38145825e-02,  2.38800030e-02,\n",
       "          8.95911735e-03,  8.24760273e-03,  9.20477044e-03,\n",
       "         -2.09759432e-03, -4.36333194e-03,  3.39965057e-03,\n",
       "          2.53781886e-03,  5.45814866e-03,  1.06988847e-03,\n",
       "         -5.15932590e-03,  8.64498690e-03,  6.97287824e-03,\n",
       "          6.05070777e-03],\n",
       "        [-2.72740057e-04, -1.55722242e-04,  1.15927437e-03,\n",
       "          3.75898584e-04,  1.25621026e-03,  3.47896304e-04,\n",
       "         -1.09839567e-03,  3.34226806e-03,  2.52912100e-03,\n",
       "          3.53652635e-03, -2.52739468e-04, -1.53014658e-03,\n",
       "          4.65103629e-04,  3.51820199e-04,  4.40480333e-04,\n",
       "          7.29855456e-05, -8.96007637e-04,  5.42633992e-04,\n",
       "          5.55361388e-04,  5.70158998e-04,  1.06334537e-02,\n",
       "          1.00794518e-02,  7.15684984e-03,  1.18137188e-02,\n",
       "          4.81955614e-03,  7.14931870e-03,  1.23583730e-02,\n",
       "          4.63652005e-03,  4.26829699e-03,  4.76365024e-03,\n",
       "         -1.08554633e-03, -2.25811033e-03,  1.75938627e-03,\n",
       "          1.31337135e-03,  2.82469951e-03,  5.53688384e-04,\n",
       "         -2.67005293e-03,  4.47395118e-03,  3.60860210e-03,\n",
       "          3.13136075e-03],\n",
       "        [ 1.72759534e-03,  9.86378873e-04, -7.34309806e-03,\n",
       "         -2.38102395e-03, -7.95711111e-03, -2.20365147e-03,\n",
       "          6.95747882e-03, -2.11706590e-02, -1.60200074e-02,\n",
       "         -2.24011317e-02,  1.60090718e-03,  9.69228335e-03,\n",
       "         -2.94606830e-03, -2.22850638e-03, -2.79009901e-03,\n",
       "         -4.62306460e-04,  5.67550864e-03, -3.43716284e-03,\n",
       "         -3.51778069e-03, -3.61151202e-03, -6.73546270e-02,\n",
       "         -6.38454556e-02, -4.53330539e-02, -7.48306811e-02,\n",
       "         -3.05281244e-02, -4.52853516e-02, -7.82806426e-02,\n",
       "         -2.93687340e-02, -2.70363279e-02, -3.01740039e-02,\n",
       "          6.87608868e-03,  1.43033648e-02, -1.11443382e-02,\n",
       "         -8.31918232e-03, -1.78922657e-02, -3.50718363e-03,\n",
       "          1.69127006e-02, -2.83389874e-02, -2.28576753e-02,\n",
       "         -1.98347252e-02],\n",
       "        [-1.14537845e-03, -6.53959287e-04,  4.86840028e-03,\n",
       "          1.57859514e-03,  5.27548511e-03,  1.46099890e-03,\n",
       "         -4.61273873e-03,  1.40359346e-02,  1.06211044e-02,\n",
       "          1.48517266e-02, -1.06138550e-03, -6.42588688e-03,\n",
       "          1.95321394e-03,  1.47747737e-03,  1.84980780e-03,\n",
       "          3.06504546e-04, -3.76280560e-03,  2.27880455e-03,\n",
       "          2.33225338e-03,  2.39439635e-03,  4.46554422e-02,\n",
       "          4.23288979e-02,  3.00553627e-02,  4.96119969e-02,\n",
       "          2.02398412e-02,  3.00237369e-02,  5.18992804e-02,\n",
       "          1.94711778e-02,  1.79248154e-02,  2.00050622e-02,\n",
       "         -4.55877790e-03, -9.48298816e-03,  7.38858478e-03,\n",
       "          5.51553443e-03,  1.18623935e-02,  2.32522772e-03,\n",
       "         -1.12129515e-02,  1.87884644e-02,  1.51544111e-02,\n",
       "          1.31502245e-02],\n",
       "        [-7.20181852e-04, -4.11191286e-04,  3.06111365e-03,\n",
       "          9.92576359e-04,  3.31707695e-03,  9.18635167e-04,\n",
       "         -2.90036085e-03,  8.82540271e-03,  6.67825295e-03,\n",
       "          9.33834910e-03, -6.67369401e-04, -4.04041773e-03,\n",
       "          1.22812612e-03,  9.28996364e-04,  1.16310723e-03,\n",
       "          1.92721476e-04, -2.36594654e-03,  1.43284840e-03,\n",
       "          1.46645552e-03,  1.50552928e-03,  2.80780904e-02,\n",
       "          2.66152248e-02,  1.88979693e-02,  3.11946310e-02,\n",
       "          1.27262445e-02,  1.88780837e-02,  3.26328129e-02,\n",
       "          1.22429300e-02,  1.12706209e-02,  1.25786224e-02,\n",
       "         -2.86643184e-03, -5.96263679e-03,  4.64573503e-03,\n",
       "          3.46801337e-03,  7.45874038e-03,  1.46203802e-03,\n",
       "         -7.05038942e-03,  1.18136602e-02,  9.52866860e-03,\n",
       "          8.26849230e-03],\n",
       "        [-8.66642687e-04, -4.94813838e-04,  3.68364179e-03,\n",
       "          1.19443319e-03,  3.99165927e-03,  1.10545475e-03,\n",
       "         -3.49019724e-03,  1.06201936e-02,  8.03638622e-03,\n",
       "          1.12374574e-02, -8.03090050e-04, -4.86210315e-03,\n",
       "          1.47788587e-03,  1.11792306e-03,  1.39964430e-03,\n",
       "          2.31914571e-04, -2.84710107e-03,  1.72424177e-03,\n",
       "          1.76468352e-03,  1.81170355e-03,  3.37882340e-02,\n",
       "          3.20278704e-02,  2.27411836e-02,  3.75385769e-02,\n",
       "          1.53143369e-02,  2.27172542e-02,  3.92692350e-02,\n",
       "          1.47327324e-02,  1.35626886e-02,  1.51366936e-02,\n",
       "         -3.44936806e-03, -7.17523787e-03,  5.59052173e-03,\n",
       "          4.17329138e-03,  8.97559896e-03,  1.75936753e-03,\n",
       "         -8.48420244e-03,  1.42161632e-02,  1.14664808e-02,\n",
       "          9.95002687e-03]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(40,), dtype=float32, numpy=\n",
       " array([-0.09601873, -0.3651618 ,  0.20979817,  0.06284291,  0.14610083,\n",
       "         0.06006461, -0.27875867,  0.25046563,  0.20975704,  0.27237064,\n",
       "        -0.01280924, -0.07755028,  0.0235722 ,  0.01783081,  0.02232425,\n",
       "         0.00369902, -0.0454111 ,  0.02750156,  0.0281466 ,  0.02889657,\n",
       "         1.3454621 ,  0.84417355,  1.2938945 ,  1.8149537 ,  0.5718073 ,\n",
       "         0.83917165,  1.4247936 ,  1.44938   ,  0.6097311 ,  1.200451  ,\n",
       "        -0.09839188, -0.265769  ,  0.15980157,  0.11609915,  0.20449454,\n",
       "         0.03828417, -0.2543427 ,  0.29469424,  0.23524943,  0.22041392],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "447a3847-162c-40c5-91aa-4387850d7f7d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 10), dtype=float32, numpy=\n",
       "array([[[-0.04457429, -0.20452513,  0.07552136,  0.0508462 ,\n",
       "          0.03812613,  0.01973103, -0.12498064,  0.08286092,\n",
       "          0.05210062,  0.06269614],\n",
       "        [-0.05648873, -0.13043042,  0.1044731 ,  0.06710292,\n",
       "          0.10755109,  0.02488116, -0.18320872,  0.2810727 ,\n",
       "          0.16890085,  0.2183119 ]],\n",
       "\n",
       "       [[-0.04457429, -0.20452513,  0.07552136,  0.0508462 ,\n",
       "          0.03812613,  0.01973103, -0.12498064,  0.08286092,\n",
       "          0.05210062,  0.06269614],\n",
       "        [-0.05648873, -0.13043042,  0.1044731 ,  0.06710292,\n",
       "          0.10755109,  0.02488116, -0.18320872,  0.2810727 ,\n",
       "          0.16890085,  0.2183119 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e586409-6d87-463c-90dd-40135c565e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1 like in Jonathan Hui pdf\n",
    "class Bahdau_attention(layers.Layer):\n",
    "    def __init__(self, units = 10):\n",
    "        super().__init__()\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, decoder_hidden, encoder_hidden):\n",
    "        # decoder_hidden.shape = (batch_size, hidden_size)\n",
    "        # decoder_hidden_time_axis.shape = (batch_size, 1, hidden_size)\n",
    "        decoder_hidden_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "        \n",
    "        # encoder_hidden.shape = (batch_size, max_sen_len, hidden_size)\n",
    "        # argument for tanh shape = (batch_size, max_sen_len, hidden_size)\n",
    "        # score.shape = (batch_size, max_sen_len, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(decoder_hidden_time_axis) + self.W2(encoder_hidden)))\n",
    "        \n",
    "        # attention_weights.shape = (batch_size, max_sen_len, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
    "        \n",
    "        # context_vector.shape = (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * encoder_hidden\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69eb4b57-421e-4ad8-91fa-c445ee520f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2, like in attention explanation pdf\n",
    "class Bahdau_attention(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = layers.Dense(1)\n",
    "       \n",
    "    @tf.function\n",
    "    def call(self, decoder_state_h, encoder_states_h):\n",
    "        # decoder_state_h.shape = (batch_size, dec_state_size)\n",
    "        # encoder_states_h.shape = (batch_size, max_sen_len, enc_state_size)\n",
    "        \n",
    "        # make sure that the dtypes are correct\n",
    "        decoder_state_h = tf.cast(decoder_state_h, tf.float32)\n",
    "        encoder_states_h = tf.cast(encoder_states_h, tf.float32)\n",
    "        \n",
    "        # encoder_states_h_flattened.shape = (batch_size * max_sen_len, enc_state_size)\n",
    "        # encoder_states_h_flattened = tf.reshape(encoder_states_h, [-1, tf.shape(encoder_states_h)[2]])\n",
    "        encoder_states_h_flattened = tf.reshape(\n",
    "            encoder_states_h, [\n",
    "                tf.shape(encoder_states_h)[0] * tf.shape(encoder_states_h)[1], \n",
    "                tf.shape(encoder_states_h)[2]\n",
    "            ]\n",
    "        )\n",
    "        batch_size = tf.shape(encoder_states_h)[0]\n",
    "        max_sen_len = tf.shape(encoder_states_h)[1]\n",
    "        enc_state_size = tf.shape(encoder_states_h)[2]\n",
    "        \n",
    "        context_vector = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True, clear_after_read = False)\n",
    "        for b in tf.range(batch_size):\n",
    "            alpha = e = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True, clear_after_read = False)\n",
    "            for j in tf.range(max_sen_len):\n",
    "                # x.shape = (dec_state_size + enc_state_size)\n",
    "                x = tf.concat([decoder_state_h[b], encoder_states_h_flattened[b * max_sen_len + j]], 0)\n",
    "                # x.shape = (1, dec_state_size + enc_state_size)\n",
    "                x = tf.expand_dims(x, 0)\n",
    "                e =e.write(j, tf.math.exp(self.dense(x))[0])\n",
    "\n",
    "            e_sum = tf.math.reduce_sum(e.stack())\n",
    "            for j in tf.range(max_sen_len):\n",
    "                alpha = alpha.write(j, tf.math.divide(e.read(j), e_sum))\n",
    "\n",
    "            Sum = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size = True, clear_after_read = False)\n",
    "            for j in tf.range(max_sen_len):\n",
    "                Sum = Sum.write(j, alpha.read(j) * encoder_states_h_flattened[b * max_sen_len + j])\n",
    "\n",
    "            # context_vector_b is a context vector for 1 sample from batch\n",
    "            context_vector_b = tf.math.reduce_sum(Sum.stack(), axis = 0)\n",
    "            # context_vector.shape = (batch_size, enc_state_size)\n",
    "            context_vector = context_vector.write(b, context_vector_b)\n",
    "        \n",
    "        return context_vector.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4abc0a9f-5f9e-4c5f-8ee0-d29769dafa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.run_functions_eagerly(True)\n",
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a9ec3a1b-a11a-496d-8cb8-95960056ab17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention = Bahdau_attention()\n",
    "decoder_state_h = tf.constant([[1,2], [4,5]])\n",
    "encoder_states_h = tf.constant([[[1,2,3], [4,5,3]], [[1,2,3], [4,5,3]]])\n",
    "encoder_states_h_flattened = tf.reshape(encoder_states_h, [tf.shape(encoder_states_h)[0] * tf.shape(encoder_states_h)[1], tf.shape(encoder_states_h)[2]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    context_vector = attention(decoder_state_h, encoder_states_h)\n",
    "    \n",
    "variables = attention.trainable_variables\n",
    "gradients = tape.gradient(context_vector, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "63891f45-e849-4848-80a9-da55a4b71126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[3.5908003, 4.5908003, 3.       ],\n",
       "       [3.5908003, 4.5908003, 3.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d5307f81-c5f4-4cf3-b0d4-6a4c2bf50631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       " array([[-1.1920929e-06],\n",
       "        [-2.1457672e-06],\n",
       "        [ 4.2406158e+00],\n",
       "        [ 4.2406149e+00],\n",
       "        [-3.0994415e-06]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.013279e-06], dtype=float32)>]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a6c0cc-731a-475c-bc69-bea3475e4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_out_size, embed_matrix):\n",
    "        super().__init__()\n",
    "        self.lstm_out_size = lstm_out_size\n",
    "        self.embedding = layers.Embedding(\n",
    "            input_dim = embed_matrix.shape[0],\n",
    "            output_dim = embedding_dim,\n",
    "            embeddings_initializer = tf.keras.initializers.Constant(embed_matrix),\n",
    "            trainable = False\n",
    "        )\n",
    "        self.lstm = layers.LSTM(\n",
    "            units = self.lstm_out_size,\n",
    "            # return_sequences = True,\n",
    "            return_state = True\n",
    "        )\n",
    "        self.dense = layers.Dense(vocab_size)\n",
    "        self.attention = Bahdau_attention()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x, decoder_state_h, decoder_state_c, encoder_states_h):\n",
    "        # x.shape = (batch_size, 1)\n",
    "        # x is a single number for each batch representing a single word\n",
    "        # encoder_states_h.shape = (batch_size, max_sen_len, enc_state_size)\n",
    "        # decoder_state_h.shape = (batch_size, lstm_out_size)\n",
    "        \n",
    "        # make sure that the types are correct\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        decoder_state_h = tf.cast(decoder_state_h, tf.float32)\n",
    "        decoder_state_c = tf.cast(decoder_state_c, tf.float32)\n",
    "        encoder_states_h = tf.cast(encoder_states_h, tf.float32)\n",
    "        \n",
    "        # context_vector.shape = (batch_size, enc_state_size)\n",
    "        context_vector = self.attention(decoder_state_h, encoder_states_h)\n",
    "        # shape of output of embedding layer = (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # x.shape after concatenation = (batch_size, 1, enc_state_size + embedding_dim)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = 2)\n",
    "        \n",
    "        output, state_h, state_c = self.lstm(x, initial_state = [decoder_state_h, decoder_state_c])\n",
    "        \n",
    "        # output.shape = (batch_size, vocab_size)\n",
    "        output = self.dense(output)\n",
    "        \n",
    "        return output, state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0bfb23f3-bb93-4967-9b92-8bdf3247ab4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_size = 100, \n",
    "                  embedding_dim = 300, \n",
    "                  lstm_out_size = 20, \n",
    "                  embed_matrix = embed_matrix\n",
    "                 )\n",
    "\n",
    "x = tf.constant([[1], [2]])\n",
    "decoder_state_h = tf.constant([[i for i in range(20)], [i for i in range(20)]])\n",
    "decoder_state_c = tf.constant([[i for i in range(20)], [i for i in range(20)]])\n",
    "encoder_states_h = tf.constant([[[i for i in range(15)], [i for i in range(15)]], [[i for i in range(15)], [i for i in range(15)]]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    output, state_h, state_c = decoder(x, decoder_state_h, decoder_state_c, encoder_states_h)\n",
    "    \n",
    "variables = decoder.trainable_variables\n",
    "gradients = tape.gradient(output, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "38e7a82a-d985-433a-8a7b-0ee95bc9447e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 100), dtype=float32, numpy=\n",
       "array([[ 0.15162158, -0.2666756 , -0.08206277,  0.71664655,  0.15774032,\n",
       "        -0.26463896, -0.02485025, -0.16136312,  0.32311562,  0.31768876,\n",
       "        -0.38081762, -0.59177405, -0.35685304, -0.5143362 ,  0.37344497,\n",
       "        -0.51701427,  0.24112938,  0.3559965 , -0.23429905, -0.93242985,\n",
       "         0.14618877,  0.46973363, -0.7733953 ,  0.16248974,  0.03708994,\n",
       "        -0.05201417,  0.17080984,  0.08245593, -0.53828734,  0.5289846 ,\n",
       "        -0.7155414 , -0.2436806 ,  0.09993283, -0.64873123, -0.38315713,\n",
       "         0.4200229 ,  0.02848363, -0.07834591,  0.26870382,  0.37282366,\n",
       "        -0.86660933,  0.79235697, -0.47088167,  0.46041852,  0.12979311,\n",
       "         0.30807993,  0.3242702 , -0.0882818 , -0.298767  ,  0.15185268,\n",
       "         0.4537142 ,  0.4380744 ,  0.50783145,  0.2062283 ,  0.416616  ,\n",
       "        -0.24646498, -0.66551596,  0.10805789, -0.4846673 ,  0.11572916,\n",
       "         0.79025435,  0.10220787,  0.6163452 , -0.22814785, -0.5057447 ,\n",
       "         0.39544344, -0.06590679,  0.60514987, -0.00322172,  0.26729006,\n",
       "        -0.32098946, -0.11905575,  0.18295452,  0.28115597, -0.18187518,\n",
       "        -0.9423108 , -0.671106  , -0.38054192,  0.41401356, -0.31121212,\n",
       "         0.09302792,  0.02328775,  0.20752177,  0.45681006,  0.407726  ,\n",
       "        -0.48006344, -0.257822  ,  0.02856378, -0.14446467, -0.2501529 ,\n",
       "         0.31287432,  0.3248075 , -0.39628854, -0.30306733,  0.23548001,\n",
       "         0.09679663, -0.15450694,  0.5000636 , -0.04544456, -0.136026  ],\n",
       "       [ 0.14962938, -0.25731224, -0.1020506 ,  0.74165434,  0.16988227,\n",
       "        -0.25353545, -0.00930303, -0.18302968,  0.3293348 ,  0.33174652,\n",
       "        -0.40506244, -0.6096351 , -0.3950976 , -0.51407075,  0.40101132,\n",
       "        -0.53554595,  0.26527983,  0.34992194, -0.26417896, -0.9675321 ,\n",
       "         0.1488972 ,  0.47112727, -0.75738275,  0.17284009,  0.0447695 ,\n",
       "        -0.04068142,  0.172411  ,  0.10267399, -0.5499263 ,  0.5376295 ,\n",
       "        -0.748915  , -0.25476432,  0.0794469 , -0.6706539 , -0.3563583 ,\n",
       "         0.43546838,  0.03883401, -0.03957553,  0.2520967 ,  0.3544025 ,\n",
       "        -0.89587873,  0.81048834, -0.5096285 ,  0.479815  ,  0.14078659,\n",
       "         0.2999275 ,  0.3286984 , -0.09203713, -0.2861164 ,  0.14929047,\n",
       "         0.4844275 ,  0.43933004,  0.49119318,  0.19093968,  0.41867816,\n",
       "        -0.26108307, -0.6747335 ,  0.12904064, -0.50364935,  0.11115403,\n",
       "         0.78541887,  0.12439601,  0.6429752 , -0.23661476, -0.5237884 ,\n",
       "         0.39356777, -0.06530368,  0.6376973 ,  0.00318795,  0.26436985,\n",
       "        -0.33688334, -0.13508376,  0.19329348,  0.2740103 , -0.21423192,\n",
       "        -0.9375808 , -0.7042506 , -0.39134783,  0.4334551 , -0.33300954,\n",
       "         0.08642653,  0.02045148,  0.19888389,  0.46990553,  0.42168614,\n",
       "        -0.5161682 , -0.27263558,  0.02933484, -0.12576477, -0.27693444,\n",
       "         0.3377708 ,  0.2977826 , -0.39529914, -0.29846144,  0.22236711,\n",
       "         0.09808898, -0.15875454,  0.5058714 , -0.05592252, -0.1391761 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "df33af93-876e-474b-bedb-cbd64497f3ca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(315, 80), dtype=float32, numpy=\n",
       " array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.05591097e-02,  5.87581750e-03,  4.36880291e-02, ...,\n",
       "         -5.95262408e-01, -2.95585115e-03, -2.01966166e-01],\n",
       "        [ 4.11182195e-02,  1.17516350e-02,  8.73760581e-02, ...,\n",
       "         -1.19052482e+00, -5.91170229e-03, -4.03932333e-01],\n",
       "        ...,\n",
       "        [-1.43110575e-02,  1.94085098e-03,  1.16645843e-02, ...,\n",
       "         -1.73727512e-01, -9.20270628e-04, -6.59215897e-02],\n",
       "        [-2.49217115e-02, -2.28512537e-04, -4.86101629e-03, ...,\n",
       "          4.93213274e-02,  1.79062117e-04,  8.75781570e-03],\n",
       "        [ 4.53935117e-02,  1.55893841e-03,  1.68263167e-02, ...,\n",
       "         -2.01263383e-01, -8.90372845e-04, -5.50800711e-02]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(20, 80), dtype=float32, numpy=\n",
       " array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 2.0559110e-02,  5.8758175e-03,  4.3688029e-02, ...,\n",
       "         -5.9526241e-01, -2.9558511e-03, -2.0196617e-01],\n",
       "        [ 4.1118219e-02,  1.1751635e-02,  8.7376058e-02, ...,\n",
       "         -1.1905248e+00, -5.9117023e-03, -4.0393233e-01],\n",
       "        ...,\n",
       "        [ 3.4950486e-01,  9.9888898e-02,  7.4269652e-01, ...,\n",
       "         -1.0119461e+01, -5.0249469e-02, -3.4334247e+00],\n",
       "        [ 3.7006390e-01,  1.0576472e-01,  7.8638458e-01, ...,\n",
       "         -1.0714724e+01, -5.3205319e-02, -3.6353910e+00],\n",
       "        [ 3.9062309e-01,  1.1164053e-01,  8.3007258e-01, ...,\n",
       "         -1.1309986e+01, -5.6161173e-02, -3.8373570e+00]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(80,), dtype=float32, numpy=\n",
       " array([ 2.05591097e-02,  5.87581750e-03,  4.36880291e-02,  6.54117612e-04,\n",
       "        -1.54203153e-05, -3.36056382e-06, -2.19798267e-07, -6.86364743e-08,\n",
       "         2.84002596e-07,  0.00000000e+00,  0.00000000e+00, -4.70721570e-04,\n",
       "         0.00000000e+00,  0.00000000e+00,  3.42125550e-09,  0.00000000e+00,\n",
       "         0.00000000e+00,  8.29843993e-05, -8.79508880e-05,  1.03225615e-08,\n",
       "         0.00000000e+00, -2.61173695e-01, -7.71053433e-02, -1.55263394e-02,\n",
       "        -1.73119770e-04, -8.13165784e-08, -1.96773181e-06, -2.39068899e-12,\n",
       "         5.97915860e-05,  0.00000000e+00,  0.00000000e+00, -1.22766485e-02,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.08334723e-06,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.85898596e-02, -3.59257869e-03, -2.23458272e-07,\n",
       "         1.19763470e+00,  0.00000000e+00, -2.73926416e-04, -4.10720066e-04,\n",
       "        -1.03427315e-04, -9.01743291e-10,  0.00000000e+00, -8.73996964e-09,\n",
       "         5.43746959e-09,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -3.75132259e-10,  0.00000000e+00,\n",
       "         0.00000000e+00, -7.02071568e-10, -8.19780456e-04, -3.00826808e-09,\n",
       "         9.67675122e-04,  3.04572191e-02, -5.28654382e-02,  5.60756191e-04,\n",
       "        -6.53695911e-02, -1.41193241e-01, -7.66144076e-04, -2.51190364e-01,\n",
       "         6.08611626e-05,  2.02129632e-02, -2.35756829e-01, -1.41495094e-01,\n",
       "         7.13929594e-01, -3.59928571e-02, -1.07512195e-02,  6.21161453e-05,\n",
       "        -5.65255154e-03, -5.95262408e-01, -2.95585115e-03, -2.01966166e-01],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(20, 100), dtype=float32, numpy=\n",
       " array([[ 0.06191017,  0.06191017,  0.06191017, ...,  0.06191017,\n",
       "          0.06191017,  0.06191017],\n",
       "        [-1.2220323 , -1.2220323 , -1.2220323 , ..., -1.2220323 ,\n",
       "         -1.2220323 , -1.2220323 ],\n",
       "        [ 0.18544804,  0.18544804,  0.18544804, ...,  0.18544804,\n",
       "          0.18544804,  0.18544804],\n",
       "        ...,\n",
       "        [ 1.298124  ,  1.298124  ,  1.298124  , ...,  1.298124  ,\n",
       "          1.298124  ,  1.298124  ],\n",
       "        [ 0.00187013,  0.00187013,  0.00187013, ...,  0.00187013,\n",
       "          0.00187013,  0.00187013],\n",
       "        [ 0.23752296,  0.23752296,  0.23752296, ...,  0.23752296,\n",
       "          0.23752296,  0.23752296]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
       " array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(35, 1), dtype=float32, numpy=\n",
       " array([[0.00000000e+00],\n",
       "        [7.10375447e-09],\n",
       "        [1.42075089e-08],\n",
       "        [2.13112639e-08],\n",
       "        [2.84150179e-08],\n",
       "        [3.55187737e-08],\n",
       "        [4.26225277e-08],\n",
       "        [4.97262818e-08],\n",
       "        [5.68300358e-08],\n",
       "        [6.39337898e-08],\n",
       "        [7.10375474e-08],\n",
       "        [7.81412979e-08],\n",
       "        [8.52450555e-08],\n",
       "        [9.23488059e-08],\n",
       "        [9.94525635e-08],\n",
       "        [1.06556314e-07],\n",
       "        [1.13660072e-07],\n",
       "        [1.20763829e-07],\n",
       "        [1.27867580e-07],\n",
       "        [1.34971330e-07],\n",
       "        [0.00000000e+00],\n",
       "        [7.10375447e-09],\n",
       "        [1.42075089e-08],\n",
       "        [2.13112639e-08],\n",
       "        [2.84150179e-08],\n",
       "        [3.55187737e-08],\n",
       "        [4.26225277e-08],\n",
       "        [4.97262818e-08],\n",
       "        [5.68300358e-08],\n",
       "        [6.39337898e-08],\n",
       "        [7.10375474e-08],\n",
       "        [7.81412979e-08],\n",
       "        [8.52450555e-08],\n",
       "        [9.23488059e-08],\n",
       "        [9.94525635e-08]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([7.1037545e-09], dtype=float32)>]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "53ffc42e-fa8e-4ca6-956a-948ae38af438",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, \n",
    "               targ, \n",
    "               # enc_state_h, \n",
    "               # enc_state_c, \n",
    "               batch_size, \n",
    "               encoder, \n",
    "               decoder, \n",
    "               loss_function, \n",
    "               optimizer):\n",
    "    # inp.shape = targ.shape (batch_size, max_sen_len)\n",
    "    # enc_state_h.shape = (batch_size, enc_state_size)\n",
    "    \n",
    "    # make sure that the types are correct\n",
    "    inp = tf.cast(inp, tf.float32)\n",
    "    targ = tf.cast(targ, tf.float32)\n",
    "    # enc_state_h = tf.cast(enc_state_h, tf.float32)\n",
    "    # enc_state_c = tf.cast(enc_state_c, tf.float32)\n",
    "    \n",
    "    batch_loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # enc_output.shape = (batch_size, max_sen_len, enc_state_size)\n",
    "        # enc_state_h.shape = (batch_size, state_size)\n",
    "        # enc_output, enc_state_h, enc_state_c = encoder(inp, enc_state_h, enc_state_c)\n",
    "        enc_output, enc_state_h, enc_state_c = encoder(inp)\n",
    "        dec_state_h = enc_state_h\n",
    "        dec_state_c = enc_state_c\n",
    "        \n",
    "        # dec_input.shape = (batch_size, 1)\n",
    "        dec_input = tf.expand_dims([0] * batch_size, 1)\n",
    "        \n",
    "        for t in range(targ.shape[1]):\n",
    "            prediction, dec_state_h, dec_state_c, = decoder(dec_input, dec_state_h, dec_state_c, enc_output)\n",
    "            # real value passed to loss_function needs to have shape (batch_size).\n",
    "            # It is a number representing a word from tokenizer.word_index. Real value = 0\n",
    "            # means that there was no word\n",
    "            batch_loss += loss_function(targ[:, t], prediction)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "            \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(batch_loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3bd15e-6e38-4b08-92b2-530b10f6742e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bc3a1-4872-4ca2-8ca7-f6e510dbdaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194725f-0785-46aa-96ea-9a53437540aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "16035f6f-2d3d-43bd-8f41-174494e45259",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction = 'none')\n",
    "\n",
    "@tf.function\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype = loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a8ddff7-2151-45af-9547-df2a605e46cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([5.1293328e-02, 1.1920928e-07], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# real = tf.expand_dims(x_train[:2, 0], 1)\n",
    "# pred = tf.expand_dims(x_train[:2, 0], 1)\n",
    "\n",
    "real = tf.constant([1, 0])\n",
    "pred = tf.constant([[0.05, 0.95], [1, 0]])\n",
    "\n",
    "real = tf.cast(real, tf.float32)\n",
    "pred = tf.cast(pred, tf.float32)\n",
    "\n",
    "loss_object(real, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "423aca49-8c11-4305-bb06-80e0091323dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 20\n",
    "embedding_dim = 300\n",
    "\n",
    "inp = tf.constant(x_train)\n",
    "targ = tf.constant(x_train)\n",
    "\n",
    "decoder = Decoder(vocab_size = len(tokenizer.word_index.keys()) + 1,\n",
    "                  embedding_dim = embedding_dim,\n",
    "                  lstm_out_size = 100,\n",
    "                  embed_matrix = embed_matrix\n",
    "                 )\n",
    "\n",
    "encoder = Encoder(embedding_dim = embedding_dim,\n",
    "                 lstm_out_size = 100,\n",
    "                 batch_size = batch_size,\n",
    "                 embed_matrix = embed_matrix\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "48974688-4e2c-4b76-b5ea-b065ea4adc39",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 0, Loss: 2.658289670944214, Time per batch: 12.892604351043701\n",
      "Batch number: 1, Loss: 1.8582338094711304, Time per batch: 6.700369834899902\n",
      "Batch number: 2, Loss: 1.9761184453964233, Time per batch: 4.62457807858785\n",
      "Batch number: 3, Loss: 1.600721001625061, Time per batch: 3.584436297416687\n",
      "Batch number: 4, Loss: 1.403429627418518, Time per batch: 2.9829472064971925\n",
      "Batch number: 5, Loss: 1.548048734664917, Time per batch: 2.6161230007807412\n",
      "Batch number: 6, Loss: 1.5573298931121826, Time per batch: 2.3571054935455322\n",
      "Batch number: 7, Loss: 1.623695731163025, Time per batch: 2.1703423857688904\n",
      "Batch number: 8, Loss: 1.5399298667907715, Time per batch: 2.016980383131239\n",
      "Batch number: 9, Loss: 1.9213107824325562, Time per batch: 1.8950824975967406\n",
      "Batch number: 10, Loss: 1.4386277198791504, Time per batch: 1.7958932356400923\n",
      "Batch number: 11, Loss: 1.1954272985458374, Time per batch: 1.7134931286176045\n",
      "Batch number: 12, Loss: 1.583144187927246, Time per batch: 1.6415338149437537\n",
      "Batch number: 13, Loss: 1.5457284450531006, Time per batch: 1.5810672725949968\n",
      "Batch number: 14, Loss: 1.6652863025665283, Time per batch: 1.5266628901163737\n",
      "Batch number: 15, Loss: 1.455956220626831, Time per batch: 1.4799965173006058\n",
      "Batch number: 16, Loss: 1.6427024602890015, Time per batch: 1.4392958388609045\n",
      "Batch number: 17, Loss: 1.425571322441101, Time per batch: 1.402501596344842\n",
      "Batch number: 18, Loss: 1.2207692861557007, Time per batch: 1.3697961506090666\n",
      "Batch number: 19, Loss: 1.5392529964447021, Time per batch: 1.3406060099601746\n",
      "Batch number: 20, Loss: 1.7452634572982788, Time per batch: 1.3134347257160006\n",
      "Batch number: 21, Loss: 1.2616329193115234, Time per batch: 1.2881873087449507\n",
      "Batch number: 22, Loss: 1.2576507329940796, Time per batch: 1.2664401116578474\n",
      "Batch number: 23, Loss: 1.1084522008895874, Time per batch: 1.2457134226957958\n",
      "Batch number: 24, Loss: 1.2948755025863647, Time per batch: 1.2271249771118165\n",
      "Batch number: 25, Loss: 1.2437546253204346, Time per batch: 1.2100433019491343\n",
      "Batch number: 26, Loss: 1.0863875150680542, Time per batch: 1.1939676779287833\n",
      "Batch number: 27, Loss: 1.3605527877807617, Time per batch: 1.178540289402008\n",
      "Batch number: 28, Loss: 1.4847205877304077, Time per batch: 1.164816223341843\n",
      "Batch number: 29, Loss: 1.1594340801239014, Time per batch: 1.1518585681915283\n",
      "Batch number: 30, Loss: 1.2384836673736572, Time per batch: 1.139927702565347\n",
      "Batch number: 31, Loss: 1.0625183582305908, Time per batch: 1.1290549859404564\n",
      "Epoch: 1, Loss: 1.4907281398773193, Time per epoch: 36.129759550094604\n",
      "\n",
      "Batch number: 0, Loss: 1.193213701248169, Time per batch: 0.7800018787384033\n",
      "Batch number: 1, Loss: 0.9641679525375366, Time per batch: 0.778001070022583\n",
      "Batch number: 2, Loss: 1.1126807928085327, Time per batch: 0.7790014743804932\n",
      "Batch number: 3, Loss: 0.9150984883308411, Time per batch: 0.7802513837814331\n",
      "Batch number: 4, Loss: 0.8119829297065735, Time per batch: 0.7786015033721924\n",
      "Batch number: 5, Loss: 1.0941020250320435, Time per batch: 0.7895030577977499\n",
      "Batch number: 6, Loss: 1.101337194442749, Time per batch: 0.7891442094530378\n",
      "Batch number: 7, Loss: 1.2160342931747437, Time per batch: 0.7871262729167938\n",
      "Batch number: 8, Loss: 1.1505569219589233, Time per batch: 0.7873347335391574\n",
      "Batch number: 9, Loss: 1.7098039388656616, Time per batch: 0.7863013505935669\n",
      "Batch number: 10, Loss: 1.145498514175415, Time per batch: 0.7852756543592974\n",
      "Batch number: 11, Loss: 0.7986812591552734, Time per batch: 0.7860869963963827\n",
      "Batch number: 12, Loss: 1.0560321807861328, Time per batch: 0.7852350381704477\n",
      "Batch number: 13, Loss: 1.0938471555709839, Time per batch: 0.7850819144930158\n",
      "Batch number: 14, Loss: 1.1763352155685425, Time per batch: 0.7844765186309814\n",
      "Batch number: 15, Loss: 0.9916952252388, Time per batch: 0.783446803689003\n",
      "Batch number: 16, Loss: 1.0750694274902344, Time per batch: 0.7825382597306195\n",
      "Batch number: 17, Loss: 1.0392900705337524, Time per batch: 0.781452841228909\n",
      "Batch number: 18, Loss: 0.8402602076530457, Time per batch: 0.7809554275713468\n",
      "Batch number: 19, Loss: 1.1360294818878174, Time per batch: 0.7815577149391174\n",
      "Batch number: 20, Loss: 1.3326141834259033, Time per batch: 0.7824359621320452\n",
      "Batch number: 21, Loss: 0.8507593274116516, Time per batch: 0.7828252965753729\n",
      "Batch number: 22, Loss: 0.8469738960266113, Time per batch: 0.7833981824957806\n",
      "Batch number: 23, Loss: 0.8074896931648254, Time per batch: 0.7834232648213705\n",
      "Batch number: 24, Loss: 0.8746035695075989, Time per batch: 0.7830063819885253\n",
      "Batch number: 25, Loss: 0.9467917680740356, Time per batch: 0.7822003456262442\n",
      "Batch number: 26, Loss: 0.8499283790588379, Time per batch: 0.782563324327822\n",
      "Batch number: 27, Loss: 1.000626564025879, Time per batch: 0.7827942456517901\n",
      "Batch number: 28, Loss: 1.0720250606536865, Time per batch: 0.7820111800884378\n",
      "Batch number: 29, Loss: 0.8759832382202148, Time per batch: 0.7815441926320393\n",
      "Batch number: 30, Loss: 0.8849241137504578, Time per batch: 0.7814651381584906\n",
      "Batch number: 31, Loss: 0.782039999961853, Time per batch: 0.7817943766713142\n",
      "Epoch: 2, Loss: 1.0233274698257446, Time per epoch: 25.017420053482056\n",
      "\n",
      "Batch number: 0, Loss: 0.9319738149642944, Time per batch: 0.7751002311706543\n",
      "Batch number: 1, Loss: 0.6633359789848328, Time per batch: 0.7795451879501343\n",
      "Batch number: 2, Loss: 0.8254951238632202, Time per batch: 0.779030958811442\n",
      "Batch number: 3, Loss: 0.6655469536781311, Time per batch: 0.8002753257751465\n",
      "Batch number: 4, Loss: 0.6450530886650085, Time per batch: 0.8074206829071044\n",
      "Batch number: 5, Loss: 0.9435710906982422, Time per batch: 0.8023508787155151\n",
      "Batch number: 6, Loss: 1.1062647104263306, Time per batch: 0.79801835332598\n",
      "Batch number: 7, Loss: 0.9758796691894531, Time per batch: 0.7957662045955658\n",
      "Batch number: 8, Loss: 0.8296539187431335, Time per batch: 0.7951245043012831\n",
      "Batch number: 9, Loss: 1.3229138851165771, Time per batch: 0.7949122428894043\n",
      "Batch number: 10, Loss: 0.9234673380851746, Time per batch: 0.7948294552889738\n",
      "Batch number: 11, Loss: 0.6387056708335876, Time per batch: 0.7943437496821085\n",
      "Batch number: 12, Loss: 0.8823797106742859, Time per batch: 0.7941659047053411\n",
      "Batch number: 13, Loss: 0.8332313299179077, Time per batch: 0.7936540842056274\n",
      "Batch number: 14, Loss: 1.0511971712112427, Time per batch: 0.7950470129648844\n",
      "Batch number: 15, Loss: 0.7244209051132202, Time per batch: 0.7946066707372665\n",
      "Batch number: 16, Loss: 0.9041229486465454, Time per batch: 0.7959828236523796\n",
      "Batch number: 17, Loss: 0.8199657201766968, Time per batch: 0.7962632576624552\n",
      "Batch number: 18, Loss: 0.8426198959350586, Time per batch: 0.7962494900352076\n",
      "Batch number: 19, Loss: 0.9760439991950989, Time per batch: 0.7961870789527893\n",
      "Batch number: 20, Loss: 1.1119072437286377, Time per batch: 0.7966548942384266\n",
      "Batch number: 21, Loss: 0.9104726910591125, Time per batch: 0.7961701479825106\n",
      "Batch number: 22, Loss: 0.9481269717216492, Time per batch: 0.7959454266921334\n",
      "Batch number: 23, Loss: 1.00713312625885, Time per batch: 0.7954180240631104\n",
      "Batch number: 24, Loss: 1.1516375541687012, Time per batch: 0.7953213691711426\n",
      "Batch number: 25, Loss: 1.0572227239608765, Time per batch: 0.7954628926057082\n",
      "Batch number: 26, Loss: 0.9010038375854492, Time per batch: 0.7975939291494863\n",
      "Batch number: 27, Loss: 0.8980768322944641, Time per batch: 0.7993585126740592\n",
      "Batch number: 28, Loss: 1.0417978763580322, Time per batch: 0.7999669190110832\n",
      "Batch number: 29, Loss: 0.7831605672836304, Time per batch: 0.8014347076416015\n",
      "Batch number: 30, Loss: 0.8380144238471985, Time per batch: 0.8023884373326455\n",
      "Batch number: 31, Loss: 0.8230887651443481, Time per batch: 0.803220123052597\n",
      "Epoch: 3, Loss: 0.9055465459823608, Time per epoch: 25.703043937683105\n",
      "\n",
      "Batch number: 0, Loss: 0.7025620341300964, Time per batch: 0.8160011768341064\n",
      "Batch number: 1, Loss: 0.6907938718795776, Time per batch: 0.8430013656616211\n",
      "Batch number: 2, Loss: 0.7685742378234863, Time per batch: 0.8336676756540934\n",
      "Batch number: 3, Loss: 0.6884898543357849, Time per batch: 0.8240038752555847\n",
      "Batch number: 4, Loss: 0.563175618648529, Time per batch: 0.821601390838623\n",
      "Batch number: 5, Loss: 0.8178912997245789, Time per batch: 0.8208362261454264\n",
      "Batch number: 6, Loss: 1.1273919343948364, Time per batch: 0.8217169557298932\n",
      "Batch number: 7, Loss: 0.8331775665283203, Time per batch: 0.8197514116764069\n",
      "Batch number: 8, Loss: 0.7334402203559875, Time per batch: 0.8193346394432915\n",
      "Batch number: 9, Loss: 1.1814042329788208, Time per batch: 0.822002363204956\n",
      "Batch number: 10, Loss: 0.9033918380737305, Time per batch: 0.8246879144148394\n",
      "Batch number: 11, Loss: 0.5753828287124634, Time per batch: 0.8238797982533773\n",
      "Batch number: 12, Loss: 0.8266841769218445, Time per batch: 0.8226584287790152\n",
      "Batch number: 13, Loss: 0.9042066335678101, Time per batch: 0.8213650158473423\n",
      "Batch number: 14, Loss: 0.962816596031189, Time per batch: 0.8204074541727702\n",
      "Batch number: 15, Loss: 0.8693529963493347, Time per batch: 0.820256382226944\n",
      "Batch number: 16, Loss: 1.0031530857086182, Time per batch: 0.8197708410375258\n",
      "Batch number: 17, Loss: 0.9356285929679871, Time per batch: 0.8198947774039375\n",
      "Batch number: 18, Loss: 0.741168200969696, Time per batch: 0.8197424286290219\n",
      "Batch number: 19, Loss: 0.8272620439529419, Time per batch: 0.8194553852081299\n",
      "Batch number: 20, Loss: 1.0935560464859009, Time per batch: 0.819100470769973\n",
      "Batch number: 21, Loss: 0.824047863483429, Time per batch: 0.8191859722137451\n",
      "Batch number: 22, Loss: 0.8333589434623718, Time per batch: 0.8202649406764818\n",
      "Batch number: 23, Loss: 0.7658321261405945, Time per batch: 0.8200456301371256\n",
      "Batch number: 24, Loss: 1.0333585739135742, Time per batch: 0.8202438545227051\n",
      "Batch number: 25, Loss: 0.7495636940002441, Time per batch: 0.8206575833834134\n",
      "Batch number: 26, Loss: 0.7803261876106262, Time per batch: 0.8207814605147751\n",
      "Batch number: 27, Loss: 0.8907111883163452, Time per batch: 0.8212535892214093\n",
      "Batch number: 28, Loss: 0.966281533241272, Time per batch: 0.8211759369948815\n",
      "Batch number: 29, Loss: 0.7800542712211609, Time per batch: 0.8227612495422363\n",
      "Batch number: 30, Loss: 0.8694488406181335, Time per batch: 0.8230915838672269\n",
      "Batch number: 31, Loss: 0.8731260299682617, Time per batch: 0.8225890770554543\n",
      "Epoch: 4, Loss: 0.8473628759384155, Time per epoch: 26.322850465774536\n",
      "\n",
      "Batch number: 0, Loss: 1.0747767686843872, Time per batch: 0.820991039276123\n",
      "Batch number: 1, Loss: 0.9441061019897461, Time per batch: 0.8204964399337769\n",
      "Batch number: 2, Loss: 0.9567734599113464, Time per batch: 0.8226613998413086\n",
      "Batch number: 3, Loss: 0.9482352137565613, Time per batch: 0.8219966292381287\n",
      "Batch number: 4, Loss: 0.9420207738876343, Time per batch: 0.8247974872589111\n",
      "Batch number: 5, Loss: 1.141543984413147, Time per batch: 0.8233315149943033\n",
      "Batch number: 6, Loss: 1.0906355381011963, Time per batch: 0.8244270937783378\n",
      "Batch number: 7, Loss: 1.2305657863616943, Time per batch: 0.8236342072486877\n",
      "Batch number: 8, Loss: 0.9807275533676147, Time per batch: 0.8243416680230035\n",
      "Batch number: 9, Loss: 0.9229635000228882, Time per batch: 0.8237076520919799\n",
      "Batch number: 10, Loss: 0.8195264935493469, Time per batch: 0.8240069909529253\n",
      "Batch number: 11, Loss: 0.8737494349479675, Time per batch: 0.8234232465426127\n",
      "Batch number: 12, Loss: 1.3297879695892334, Time per batch: 0.8223081735464243\n",
      "Batch number: 13, Loss: 1.2577779293060303, Time per batch: 0.8217869315828595\n",
      "Batch number: 14, Loss: 1.1708787679672241, Time per batch: 0.8233339468638102\n",
      "Batch number: 15, Loss: 1.2801799774169922, Time per batch: 0.8261881619691849\n",
      "Batch number: 16, Loss: 1.1980148553848267, Time per batch: 0.8278842393089744\n",
      "Batch number: 17, Loss: 1.1714880466461182, Time per batch: 0.8278907537460327\n",
      "Batch number: 18, Loss: 0.8980401158332825, Time per batch: 0.8282123113933363\n",
      "Batch number: 19, Loss: 1.018897294998169, Time per batch: 0.8288018226623535\n",
      "Batch number: 20, Loss: 1.3028647899627686, Time per batch: 0.8288113049098423\n",
      "Batch number: 21, Loss: 1.1969091892242432, Time per batch: 0.8292745135047219\n",
      "Batch number: 22, Loss: 1.0975154638290405, Time per batch: 0.8284365405207095\n",
      "Batch number: 23, Loss: 1.0531320571899414, Time per batch: 0.8288767437140147\n",
      "Batch number: 24, Loss: 1.2004711627960205, Time per batch: 0.8284017276763916\n",
      "Batch number: 25, Loss: 1.0420501232147217, Time per batch: 0.828386325102586\n",
      "Batch number: 26, Loss: 1.0733095407485962, Time per batch: 0.8302249908447266\n",
      "Batch number: 27, Loss: 1.218116044998169, Time per batch: 0.8310394542557853\n",
      "Batch number: 28, Loss: 1.3146023750305176, Time per batch: 0.8311760836634142\n",
      "Batch number: 29, Loss: 0.929500937461853, Time per batch: 0.8313043912251791\n",
      "Batch number: 30, Loss: 1.0564810037612915, Time per batch: 0.8309075447820848\n",
      "Batch number: 31, Loss: 0.9917963743209839, Time per batch: 0.8304729834198952\n",
      "Epoch: 5, Loss: 1.0852324962615967, Time per epoch: 26.575135469436646\n",
      "\n",
      "Batch number: 0, Loss: 1.1812751293182373, Time per batch: 0.8140010833740234\n",
      "Batch number: 1, Loss: 0.9954471588134766, Time per batch: 0.8210715055465698\n",
      "Batch number: 2, Loss: 1.052876353263855, Time per batch: 0.8250478903452555\n",
      "Batch number: 3, Loss: 0.9284455180168152, Time per batch: 0.8220359683036804\n",
      "Batch number: 4, Loss: 0.8901592493057251, Time per batch: 0.8204291343688965\n",
      "Batch number: 5, Loss: 0.9994088411331177, Time per batch: 0.8225244681040446\n",
      "Batch number: 6, Loss: 1.0241615772247314, Time per batch: 0.8220243113381522\n",
      "Batch number: 7, Loss: 1.0911628007888794, Time per batch: 0.8213976621627808\n",
      "Batch number: 8, Loss: 0.9919525384902954, Time per batch: 0.8217970000372993\n",
      "Batch number: 9, Loss: 1.1581640243530273, Time per batch: 0.824017333984375\n",
      "Batch number: 10, Loss: 1.0145900249481201, Time per batch: 0.8275614651766691\n",
      "Batch number: 11, Loss: 0.9188739657402039, Time per batch: 0.829598049322764\n",
      "Batch number: 12, Loss: 1.0976070165634155, Time per batch: 0.8307091089395376\n",
      "Batch number: 13, Loss: 1.1783663034439087, Time per batch: 0.8338728972843715\n",
      "Batch number: 14, Loss: 1.1677356958389282, Time per batch: 0.8354141076405843\n",
      "Batch number: 15, Loss: 1.1007343530654907, Time per batch: 0.8350758701562881\n",
      "Batch number: 16, Loss: 1.1677356958389282, Time per batch: 0.835306728587431\n",
      "Batch number: 17, Loss: 1.0720196962356567, Time per batch: 0.834845330980089\n",
      "Batch number: 18, Loss: 0.9380170702934265, Time per batch: 0.8362745862258109\n",
      "Batch number: 19, Loss: 1.0564870834350586, Time per batch: 0.8377614378929138\n",
      "Batch number: 20, Loss: 1.3183491230010986, Time per batch: 0.838010447365897\n",
      "Batch number: 21, Loss: 1.2060123682022095, Time per batch: 0.8384191664782438\n",
      "Batch number: 22, Loss: 1.1103060245513916, Time per batch: 0.8405314321103303\n",
      "Batch number: 23, Loss: 1.0624480247497559, Time per batch: 0.8406343360741934\n",
      "Batch number: 24, Loss: 1.1946516036987305, Time per batch: 0.8400494194030762\n",
      "Batch number: 25, Loss: 1.0528764724731445, Time per batch: 0.8395856435482318\n",
      "Batch number: 26, Loss: 1.0604450702667236, Time per batch: 0.8396010310561569\n",
      "Batch number: 27, Loss: 1.187481164932251, Time per batch: 0.8391867535454887\n",
      "Batch number: 28, Loss: 1.2634516954421997, Time per batch: 0.8393183083369814\n",
      "Batch number: 29, Loss: 0.9284455180168152, Time per batch: 0.8389077345530193\n",
      "Batch number: 30, Loss: 1.0624480247497559, Time per batch: 0.8394600345242408\n",
      "Batch number: 31, Loss: 0.9763034582138062, Time per batch: 0.8392271995544434\n",
      "Epoch: 6, Loss: 1.076513648033142, Time per epoch: 26.855270385742188\n",
      "\n",
      "Batch number: 0, Loss: 1.1581640243530273, Time per batch: 0.8189928531646729\n",
      "Batch number: 1, Loss: 1.0145900249481201, Time per batch: 0.8262671232223511\n",
      "Batch number: 2, Loss: 1.052876353263855, Time per batch: 0.8321823279062907\n",
      "Batch number: 3, Loss: 0.9284455180168152, Time per batch: 0.8313841819763184\n",
      "Batch number: 4, Loss: 0.8901592493057251, Time per batch: 0.8327391624450684\n",
      "Batch number: 5, Loss: 1.0145900249481201, Time per batch: 0.8362877368927002\n",
      "Batch number: 6, Loss: 1.0241615772247314, Time per batch: 0.8336754526410785\n",
      "Batch number: 7, Loss: 1.118618369102478, Time per batch: 0.832466185092926\n",
      "Batch number: 8, Loss: 0.9763034582138062, Time per batch: 0.8304178449842665\n",
      "Batch number: 9, Loss: 1.1581640243530273, Time per batch: 0.8301761865615844\n",
      "Batch number: 10, Loss: 1.0145900249481201, Time per batch: 0.8300694552334872\n",
      "Batch number: 11, Loss: 0.9188739657402039, Time per batch: 0.8296472032864889\n",
      "Batch number: 12, Loss: 1.1289551258087158, Time per batch: 0.8305204831636869\n",
      "Batch number: 13, Loss: 1.2328693866729736, Time per batch: 0.8314834492547172\n",
      "Batch number: 14, Loss: 1.197105884552002, Time per batch: 0.8310513178507487\n",
      "Batch number: 15, Loss: 1.1007343530654907, Time per batch: 0.830360621213913\n",
      "Batch number: 16, Loss: 1.1677356958389282, Time per batch: 0.8309866260079777\n",
      "Batch number: 17, Loss: 1.0720196962356567, Time per batch: 0.8303763071695963\n",
      "Batch number: 18, Loss: 0.9380170702934265, Time per batch: 0.8329897930747584\n",
      "Batch number: 19, Loss: 1.0606372356414795, Time per batch: 0.8333898782730103\n",
      "Batch number: 20, Loss: 1.319909930229187, Time per batch: 0.8340638365064349\n",
      "Batch number: 21, Loss: 1.2065832614898682, Time per batch: 0.8336970372633501\n",
      "Batch number: 22, Loss: 1.1103060245513916, Time per batch: 0.8342754529870074\n",
      "Batch number: 23, Loss: 1.0624480247497559, Time per batch: 0.8370973964532217\n",
      "Batch number: 24, Loss: 1.1944420337677002, Time per batch: 0.8376935577392578\n",
      "Batch number: 25, Loss: 1.0528764724731445, Time per batch: 0.8369746299890372\n",
      "Batch number: 26, Loss: 1.059152364730835, Time per batch: 0.8368676238589816\n",
      "Batch number: 27, Loss: 1.1868810653686523, Time per batch: 0.8363367063658578\n",
      "Batch number: 28, Loss: 1.2634516954421997, Time per batch: 0.8362561669842951\n",
      "Batch number: 29, Loss: 0.9284455180168152, Time per batch: 0.8359143336613973\n",
      "Batch number: 30, Loss: 1.0624480247497559, Time per batch: 0.8362398916675199\n",
      "Batch number: 31, Loss: 0.9763034582138062, Time per batch: 0.8363886773586273\n",
      "Epoch: 7, Loss: 1.0809643268585205, Time per epoch: 26.765448331832886\n",
      "\n",
      "Batch number: 0, Loss: 1.1581640243530273, Time per batch: 0.8359997272491455\n",
      "Batch number: 1, Loss: 1.0145900249481201, Time per batch: 0.8419960737228394\n",
      "Batch number: 2, Loss: 1.052876353263855, Time per batch: 0.8419979413350424\n",
      "Batch number: 3, Loss: 0.9284455180168152, Time per batch: 0.8409988284111023\n",
      "Batch number: 4, Loss: 0.8901592493057251, Time per batch: 0.8406010627746582\n",
      "Batch number: 5, Loss: 1.0145900249481201, Time per batch: 0.8381661574045817\n",
      "Batch number: 6, Loss: 1.0241615772247314, Time per batch: 0.837428365434919\n",
      "Batch number: 7, Loss: 1.1076101064682007, Time per batch: 0.8391262888908386\n",
      "Batch number: 8, Loss: 0.9763034582138062, Time per batch: 0.8395671314663358\n",
      "Batch number: 9, Loss: 1.1581640243530273, Time per batch: 0.8379750728607178\n",
      "Batch number: 10, Loss: 1.0145900249481201, Time per batch: 0.8373421322215687\n",
      "Batch number: 11, Loss: 0.9188739657402039, Time per batch: 0.8374429146448771\n",
      "Batch number: 12, Loss: 1.1007344722747803, Time per batch: 0.837178321985098\n",
      "Batch number: 13, Loss: 1.177307367324829, Time per batch: 0.8363798345838275\n",
      "Batch number: 14, Loss: 1.1677356958389282, Time per batch: 0.8404879252115885\n",
      "Batch number: 15, Loss: 1.1007343530654907, Time per batch: 0.8403950184583664\n",
      "Batch number: 16, Loss: 1.1677356958389282, Time per batch: 0.8407836381126853\n",
      "Batch number: 17, Loss: 1.0720196962356567, Time per batch: 0.8403512901730008\n",
      "Batch number: 18, Loss: 0.9380170702934265, Time per batch: 0.8397539289374101\n",
      "Batch number: 19, Loss: 1.0337331295013428, Time per batch: 0.8387662887573242\n",
      "Batch number: 20, Loss: 1.3017381429672241, Time per batch: 0.8383489222753615\n",
      "Batch number: 21, Loss: 1.177307367324829, Time per batch: 0.8376967148347334\n",
      "Batch number: 22, Loss: 1.1103060245513916, Time per batch: 0.8371447376582933\n",
      "Batch number: 23, Loss: 1.0624480247497559, Time per batch: 0.836975634098053\n",
      "Batch number: 24, Loss: 1.1677355766296387, Time per batch: 0.8374170303344727\n",
      "Batch number: 25, Loss: 1.0528764724731445, Time per batch: 0.8379006385803223\n",
      "Batch number: 26, Loss: 1.0337331295013428, Time per batch: 0.839385880364312\n",
      "Batch number: 27, Loss: 1.1581640243530273, Time per batch: 0.8402653591973441\n",
      "Batch number: 28, Loss: 1.2634516954421997, Time per batch: 0.8421873141979349\n",
      "Batch number: 29, Loss: 0.9284455180168152, Time per batch: 0.8433141152064005\n",
      "Batch number: 30, Loss: 1.0624480247497559, Time per batch: 0.843465305143787\n",
      "Batch number: 31, Loss: 0.9763034582138062, Time per batch: 0.8441758304834366\n",
      "Epoch: 8, Loss: 1.0722343921661377, Time per epoch: 27.01362657546997\n",
      "\n",
      "Batch number: 0, Loss: 1.1581640243530273, Time per batch: 0.822075605392456\n",
      "Batch number: 1, Loss: 1.0145900249481201, Time per batch: 0.8300378322601318\n",
      "Batch number: 2, Loss: 1.052876353263855, Time per batch: 0.8362154165903727\n",
      "Batch number: 3, Loss: 0.9284455180168152, Time per batch: 0.8381617069244385\n",
      "Batch number: 4, Loss: 0.8901592493057251, Time per batch: 0.8345298290252685\n",
      "Batch number: 5, Loss: 1.0145900249481201, Time per batch: 0.8367751042048136\n",
      "Batch number: 6, Loss: 1.0241615772247314, Time per batch: 0.8376688957214355\n",
      "Batch number: 7, Loss: 1.0911628007888794, Time per batch: 0.8377126753330231\n",
      "Batch number: 8, Loss: 0.9763034582138062, Time per batch: 0.8385225401984321\n",
      "Batch number: 9, Loss: 1.1581640243530273, Time per batch: 0.8414738893508911\n",
      "Batch number: 10, Loss: 1.0145900249481201, Time per batch: 0.8435207496989857\n",
      "Batch number: 11, Loss: 0.9188739657402039, Time per batch: 0.8428108890851339\n",
      "Batch number: 12, Loss: 1.1103060245513916, Time per batch: 0.8428271183600793\n",
      "Batch number: 13, Loss: 1.1868788003921509, Time per batch: 0.8408395733152118\n",
      "Batch number: 14, Loss: 1.1677356958389282, Time per batch: 0.8402503967285156\n",
      "Batch number: 15, Loss: 1.1007343530654907, Time per batch: 0.8412973582744598\n",
      "Batch number: 16, Loss: 1.1677356958389282, Time per batch: 0.8409269557279699\n",
      "Batch number: 17, Loss: 1.0720196962356567, Time per batch: 0.8410421477423774\n",
      "Batch number: 18, Loss: 0.9380170702934265, Time per batch: 0.83980561557569\n",
      "Batch number: 19, Loss: 1.0337331295013428, Time per batch: 0.8404658675193787\n",
      "Batch number: 20, Loss: 1.3017381429672241, Time per batch: 0.8399675005958194\n",
      "Batch number: 21, Loss: 1.177307367324829, Time per batch: 0.8397427147085016\n",
      "Batch number: 22, Loss: 1.1103060245513916, Time per batch: 0.8392756918202275\n",
      "Batch number: 23, Loss: 1.0624480247497559, Time per batch: 0.839848001797994\n",
      "Batch number: 24, Loss: 1.1677355766296387, Time per batch: 0.8409745597839355\n",
      "Batch number: 25, Loss: 1.0528764724731445, Time per batch: 0.8406294400875385\n",
      "Batch number: 26, Loss: 1.0337331295013428, Time per batch: 0.8404580222235786\n",
      "Batch number: 27, Loss: 1.1581640243530273, Time per batch: 0.8403702804020473\n",
      "Batch number: 28, Loss: 1.2634516954421997, Time per batch: 0.8395645042945599\n",
      "Batch number: 29, Loss: 0.9284455180168152, Time per batch: 0.8398589134216309\n",
      "Batch number: 30, Loss: 1.0624480247497559, Time per batch: 0.8396373410378734\n",
      "Batch number: 31, Loss: 0.9763034582138062, Time per batch: 0.8400237485766411\n",
      "Epoch: 9, Loss: 1.0723187923431396, Time per epoch: 26.880759954452515\n",
      "\n",
      "Batch number: 0, Loss: 1.1581640243530273, Time per batch: 0.8610002994537354\n",
      "Batch number: 1, Loss: 1.0145900249481201, Time per batch: 0.8605014085769653\n",
      "Batch number: 2, Loss: 1.052876353263855, Time per batch: 0.8460080623626709\n",
      "Batch number: 3, Loss: 0.9284455180168152, Time per batch: 0.8460066318511963\n",
      "Batch number: 4, Loss: 0.8901592493057251, Time per batch: 0.8456053733825684\n",
      "Batch number: 5, Loss: 1.0145900249481201, Time per batch: 0.8508413235346476\n",
      "Batch number: 6, Loss: 1.0241615772247314, Time per batch: 0.8512929848262242\n",
      "Batch number: 7, Loss: 1.0911628007888794, Time per batch: 0.8496313989162445\n",
      "Batch number: 8, Loss: 0.9763034582138062, Time per batch: 0.8471115960015191\n",
      "Batch number: 9, Loss: 1.1581640243530273, Time per batch: 0.8466005802154541\n",
      "Batch number: 10, Loss: 1.0145900249481201, Time per batch: 0.846777005629106\n",
      "Batch number: 11, Loss: 0.9188739657402039, Time per batch: 0.8457123637199402\n",
      "Batch number: 12, Loss: 1.1103060245513916, Time per batch: 0.843503841987023\n",
      "Batch number: 13, Loss: 1.1868788003921509, Time per batch: 0.8447536740984235\n",
      "Batch number: 14, Loss: 1.1677356958389282, Time per batch: 0.8444037755330404\n",
      "Batch number: 15, Loss: 1.1007343530654907, Time per batch: 0.8437867015600204\n",
      "Batch number: 16, Loss: 1.1677356958389282, Time per batch: 0.8446550790001365\n",
      "Batch number: 17, Loss: 1.0720196962356567, Time per batch: 0.8438409566879272\n",
      "Batch number: 18, Loss: 0.9380170702934265, Time per batch: 0.8434283984334845\n",
      "Batch number: 19, Loss: 1.0337331295013428, Time per batch: 0.8432569980621338\n",
      "Batch number: 20, Loss: 1.3017381429672241, Time per batch: 0.8422448180970692\n",
      "Batch number: 21, Loss: 1.177307367324829, Time per batch: 0.8426221609115601\n",
      "Batch number: 22, Loss: 1.1103060245513916, Time per batch: 0.8430734302686609\n",
      "Batch number: 23, Loss: 1.0624480247497559, Time per batch: 0.8429077665011088\n",
      "Batch number: 24, Loss: 1.1677355766296387, Time per batch: 0.8423911571502686\n",
      "Batch number: 25, Loss: 1.0528764724731445, Time per batch: 0.8435892233481774\n",
      "Batch number: 26, Loss: 1.0337331295013428, Time per batch: 0.8434192957701506\n",
      "Batch number: 27, Loss: 1.1581640243530273, Time per batch: 0.8447261708123344\n",
      "Batch number: 28, Loss: 1.2634516954421997, Time per batch: 0.8448387589947931\n",
      "Batch number: 29, Loss: 0.9284455180168152, Time per batch: 0.8455441395441691\n",
      "Batch number: 30, Loss: 1.0624480247497559, Time per batch: 0.8450427978269516\n",
      "Batch number: 31, Loss: 0.9763034582138062, Time per batch: 0.8451977297663689\n",
      "Epoch: 10, Loss: 1.0723187923431396, Time per epoch: 27.047338724136353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    stime = time.time()\n",
    "    total_loss = 0\n",
    "    for batch_number in range(len(inp) // batch_size):\n",
    "        inp_batch = inp[batch_number * batch_size : (batch_number + 1) * batch_size, :]\n",
    "        targ_batch = targ[batch_number * batch_size : (batch_number + 1) * batch_size, :]\n",
    "        \n",
    "        batch_loss = train_step(inp = inp_batch,\n",
    "                               targ = targ_batch,\n",
    "                               batch_size = batch_size,\n",
    "                               encoder = encoder,\n",
    "                               decoder = decoder,\n",
    "                               loss_function = loss_function,\n",
    "                               optimizer = optimizer\n",
    "                              )\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        print(f'Batch number: {batch_number}, Loss: {batch_loss / batch_size}, Time per batch: {(time.time() - stime) / (batch_number + 1)}')\n",
    "        \n",
    "    print(f'Epoch: {epoch + 1}, Loss: {total_loss / ((batch_number + 1) * batch_size)}, Time per epoch: {time.time() - stime}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e01287fe-3046-471e-8839-c24375ec2207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/encoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/encoder\\assets\n"
     ]
    }
   ],
   "source": [
    "encoder.save('model/encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f96b89c0-8f1e-48f0-8757-63827446b636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses, dense_37_layer_call_fn, dense_37_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/decoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/decoder\\assets\n"
     ]
    }
   ],
   "source": [
    "decoder.save('model/decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a728ecf-d048-41f6-8b72-b4ffe7ea68b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564852cf-572d-4a31-9ffd-f480e106454d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1babf107-ce99-44d3-9207-feea1ca3a5de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a678df9-3be9-448c-bdff-5ef63c7f8363",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'hierarchy region business unit'\n",
    "sentence2 = 'hierarchy region'\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences([sentence1, sentence2])\n",
    "x = pad_sequences(sequences, maxlen = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80d62a0b-77d9-4074-a5bf-acc152f27562",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20 - x.shape[0]):\n",
    "    x = np.concatenate((x, np.zeros((1, 20))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "abc828e0-3b3b-4ab6-a0c2-c7f20df58db5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0., 13., 15.,  5.,  6.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0., 13., 15.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ffb6d2c-b344-4cd9-80a8-40a72613ea02",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(20, 20, 100), dtype=float32, numpy=\n",
       " array([[[-0.00200214, -0.00728252,  0.00461565, ..., -0.00156335,\n",
       "           0.00536895, -0.00102761],\n",
       "         [-0.00376189, -0.01406969,  0.00852322, ..., -0.00338283,\n",
       "           0.01094341, -0.00149374],\n",
       "         [-0.00536775, -0.02020802,  0.01205186, ..., -0.00521175,\n",
       "           0.01671277, -0.00152385],\n",
       "         ...,\n",
       "         [-0.5295285 , -0.21979973,  0.5503003 , ...,  0.03106904,\n",
       "           0.24107699, -0.02291696],\n",
       "         [-0.7674839 , -0.49447724,  0.6714796 , ..., -0.01835466,\n",
       "           0.57227296, -0.02126507],\n",
       "         [-0.82453334, -0.65076756,  0.719726  , ...,  0.03668921,\n",
       "           0.65055186, -0.06996637]],\n",
       " \n",
       "        [[-0.00200214, -0.00728252,  0.00461565, ..., -0.00156335,\n",
       "           0.00536895, -0.00102761],\n",
       "         [-0.00376189, -0.01406969,  0.00852322, ..., -0.00338283,\n",
       "           0.01094341, -0.00149374],\n",
       "         [-0.00536775, -0.02020802,  0.01205186, ..., -0.00521175,\n",
       "           0.01671277, -0.00152385],\n",
       "         ...,\n",
       "         [-0.04916742, -0.05725222,  0.08820748, ..., -0.01597076,\n",
       "           0.1287282 ,  0.01421669],\n",
       "         [-0.4745788 , -0.16935304,  0.47248846, ..., -0.03609582,\n",
       "           0.21732019, -0.0452124 ],\n",
       "         [-0.53876215, -0.22294153,  0.56159866, ...,  0.02996359,\n",
       "           0.26106238, -0.02076698]],\n",
       " \n",
       "        [[-0.00200214, -0.00728252,  0.00461565, ..., -0.00156335,\n",
       "           0.00536895, -0.00102761],\n",
       "         [-0.00376189, -0.01406969,  0.00852322, ..., -0.00338283,\n",
       "           0.01094341, -0.00149374],\n",
       "         [-0.00536775, -0.02020802,  0.01205186, ..., -0.00521175,\n",
       "           0.01671277, -0.00152385],\n",
       "         ...,\n",
       "         [-0.04916742, -0.05725222,  0.08820748, ..., -0.01597076,\n",
       "           0.1287282 ,  0.01421669],\n",
       "         [-0.05446996, -0.05849396,  0.09591879, ..., -0.01626944,\n",
       "           0.13858907,  0.01542302],\n",
       "         [-0.06014477, -0.05976397,  0.1040401 , ..., -0.01657969,\n",
       "           0.1488874 ,  0.016623  ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.00200214, -0.00728252,  0.00461565, ..., -0.00156335,\n",
       "           0.00536895, -0.00102761],\n",
       "         [-0.00376189, -0.01406969,  0.00852322, ..., -0.00338283,\n",
       "           0.01094341, -0.00149374],\n",
       "         [-0.00536775, -0.02020802,  0.01205186, ..., -0.00521175,\n",
       "           0.01671277, -0.00152385],\n",
       "         ...,\n",
       "         [-0.04916742, -0.05725222,  0.08820748, ..., -0.01597076,\n",
       "           0.1287282 ,  0.01421669],\n",
       "         [-0.05446996, -0.05849396,  0.09591879, ..., -0.01626944,\n",
       "           0.13858907,  0.01542302],\n",
       "         [-0.06014477, -0.05976397,  0.1040401 , ..., -0.01657969,\n",
       "           0.1488874 ,  0.016623  ]],\n",
       " \n",
       "        [[-0.00200214, -0.00728252,  0.00461565, ..., -0.00156335,\n",
       "           0.00536895, -0.00102761],\n",
       "         [-0.00376189, -0.01406969,  0.00852322, ..., -0.00338283,\n",
       "           0.01094341, -0.00149374],\n",
       "         [-0.00536775, -0.02020802,  0.01205186, ..., -0.00521175,\n",
       "           0.01671277, -0.00152385],\n",
       "         ...,\n",
       "         [-0.04916742, -0.05725222,  0.08820748, ..., -0.01597077,\n",
       "           0.1287282 ,  0.01421669],\n",
       "         [-0.05446995, -0.05849396,  0.09591879, ..., -0.01626945,\n",
       "           0.13858904,  0.01542302],\n",
       "         [-0.06014475, -0.05976397,  0.1040401 , ..., -0.0165797 ,\n",
       "           0.1488874 ,  0.016623  ]],\n",
       " \n",
       "        [[-0.00200214, -0.00728252,  0.00461565, ..., -0.00156335,\n",
       "           0.00536895, -0.00102761],\n",
       "         [-0.00376189, -0.01406969,  0.00852322, ..., -0.00338283,\n",
       "           0.01094341, -0.00149374],\n",
       "         [-0.00536775, -0.02020802,  0.01205186, ..., -0.00521175,\n",
       "           0.01671277, -0.00152385],\n",
       "         ...,\n",
       "         [-0.04916742, -0.05725222,  0.08820748, ..., -0.01597077,\n",
       "           0.1287282 ,  0.01421669],\n",
       "         [-0.05446995, -0.05849396,  0.09591879, ..., -0.01626945,\n",
       "           0.13858904,  0.01542302],\n",
       "         [-0.06014475, -0.05976397,  0.1040401 , ..., -0.0165797 ,\n",
       "           0.1488874 ,  0.016623  ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(20, 100), dtype=float32, numpy=\n",
       " array([[-0.82453334, -0.65076756,  0.719726  , ...,  0.03668921,\n",
       "          0.65055186, -0.06996637],\n",
       "        [-0.53876215, -0.22294153,  0.56159866, ...,  0.02996359,\n",
       "          0.26106238, -0.02076698],\n",
       "        [-0.06014477, -0.05976397,  0.1040401 , ..., -0.01657969,\n",
       "          0.1488874 ,  0.016623  ],\n",
       "        ...,\n",
       "        [-0.06014477, -0.05976397,  0.1040401 , ..., -0.01657969,\n",
       "          0.1488874 ,  0.016623  ],\n",
       "        [-0.06014475, -0.05976397,  0.1040401 , ..., -0.0165797 ,\n",
       "          0.1488874 ,  0.016623  ],\n",
       "        [-0.06014475, -0.05976397,  0.1040401 , ..., -0.0165797 ,\n",
       "          0.1488874 ,  0.016623  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(20, 100), dtype=float32, numpy=\n",
       " array([[-1.899285  , -1.2655983 ,  2.0156975 , ...,  0.15124242,\n",
       "          1.6290064 , -0.18947455],\n",
       "        [-0.9315984 , -0.405277  ,  0.982164  , ...,  0.07860176,\n",
       "          0.52806044, -0.03951072],\n",
       "        [-0.11419453, -0.11446515,  0.20176119, ..., -0.03626619,\n",
       "          0.2822519 ,  0.03376711],\n",
       "        ...,\n",
       "        [-0.11419453, -0.11446515,  0.20176119, ..., -0.03626619,\n",
       "          0.2822519 ,  0.03376711],\n",
       "        [-0.1141945 , -0.11446516,  0.20176119, ..., -0.03626619,\n",
       "          0.2822519 ,  0.03376712],\n",
       "        [-0.1141945 , -0.11446516,  0.20176119, ..., -0.03626619,\n",
       "          0.2822519 ,  0.03376712]], dtype=float32)>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80260e5-6b68-4a3c-948f-da685692c7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
